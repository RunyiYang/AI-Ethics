{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4907a9b-7f61-4e03-a118-3ca33156b516",
   "metadata": {},
   "source": [
    "# Ethics, Fairness and Explanation in AI Coursework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18977034-9183-47c7-993e-9b105ebaf8bc",
   "metadata": {},
   "source": [
    "Your goal in this coursework is to implement and experiment with various explainability approaches in order to better understand the behaviour of a neural model applied to the Titanic dataset. As you will have a chance to observe, the dataset reflects some of the past social conventions and biases, which also affect the trained model. Explanations can serve as very useful tools for identifying such potential issues and gaining insight into the internal reasoning of machine learning systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54e2fe-cc8f-4b0d-852a-474e43b5b060",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f683e86-6c7d-4ce1-9cc9-a3ec171824d8",
   "metadata": {},
   "source": [
    "We start by defining some helpful utility functions for data preprocessing. You will probably not need to change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3853c0d-4ab5-41dc-a409-ef3b7dab0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "class InvertibleColumnTransformer(ColumnTransformer):\n",
    "    \"\"\"\n",
    "    This is an invertible version of a ColumnTransformer from sklearn.\n",
    "    This allows us to recover the original feature values from their normalised\n",
    "    versions in order to better understand the produced explanations.\n",
    "    \"\"\"\n",
    "    def inverse_transform(self, X):\n",
    "        if X.ndim == 1:\n",
    "            X = np.expand_dims(X, axis=0)\n",
    "        if X.shape[1] != len(self.get_feature_names_out()):\n",
    "            raise ValueError(\n",
    "                \"X and the fitted transformer have different numbers of columns\"\n",
    "            )\n",
    "\n",
    "        inverted_X_base = np.zeros((X.shape[0], self.n_features_in_))\n",
    "        columns = [c for cs in self._columns for c in cs]\n",
    "        inverted_X = pd.DataFrame(data=inverted_X_base, columns=columns)\n",
    "        inverted_X = inverted_X.astype('object')\n",
    "        for name, indices in self.output_indices_.items():\n",
    "            transformer = self.named_transformers_.get(name, None)\n",
    "            if transformer is None:\n",
    "                continue\n",
    "\n",
    "            selected_X = X[:, indices.start : indices.stop]\n",
    "            if isinstance(transformer, OneHotEncoder):\n",
    "                # Assumed only one column changing encoder at the end\n",
    "                categories = transformer.inverse_transform(selected_X)\n",
    "                inverted_X.loc[\n",
    "                    :, columns[indices.start : indices.start + len(categories[0])]\n",
    "                ] = categories\n",
    "            else:\n",
    "                # Assumed scaler-type transformer\n",
    "                inverted_X.loc[\n",
    "                    :, [columns[i] for i in range(indices.start, indices.stop)]\n",
    "                ] = transformer.inverse_transform(selected_X)\n",
    "\n",
    "        return inverted_X\n",
    "\n",
    "\n",
    "def preprocess_train_data(\n",
    "    df,\n",
    "    scaled_features=None,\n",
    "    categorical_features=None,\n",
    "    scaler=RobustScaler(quantile_range=(10, 90)),\n",
    "    categorical_encoder=OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    Scales the continuous features using a RobustScaler and one-hot encodes\n",
    "    the categorical features.\n",
    "    \"\"\"\n",
    "    if scaled_features is None and categorical_features is None:\n",
    "        warnings.warn(\"No features specified for preprocessing, using raw data.\")\n",
    "        scaled_features = []\n",
    "        categorical_features = []\n",
    "    elif scaled_features is None:\n",
    "        scaled_features = [c for c in df.columns if c not in categorical_features]\n",
    "    elif categorical_features is None:\n",
    "        categorical_features = [c for c in df.columns if c not in scaled_features]\n",
    "\n",
    "    preprocessor = InvertibleColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", scaler, scaled_features),\n",
    "            (\"cat\", categorical_encoder, categorical_features),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "\n",
    "    preprocessed_df = preprocessor.fit_transform(df)\n",
    "    return preprocessed_df, preprocessor\n",
    "\n",
    "\n",
    "def preprocess_test_data(df, preprocessor):\n",
    "    preprocessed_df = preprocessor.transform(df)\n",
    "    return preprocessed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df35c3a2-09c0-4a14-9f83-e81db1d544a1",
   "metadata": {},
   "source": [
    "Here, we define a class for the Titanic dataset, which we will be using throughout the coursework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f7f44d-1cbe-4e25-9f7c-7bbe8c2d4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the Titanic dataset.\n",
    "    \"\"\"\n",
    "    __create_key = object()\n",
    "\n",
    "    @classmethod\n",
    "    def create_datasets(\n",
    "        cls,\n",
    "        label_name=\"survived\",\n",
    "        split_seed=42,\n",
    "        test_size=0.2,\n",
    "    ):\n",
    "        train_dataset = TitanicDataset(\n",
    "            cls.__create_key,\n",
    "            label_name=label_name,\n",
    "            split_seed=split_seed,\n",
    "            test_size=test_size,\n",
    "            train=True,\n",
    "        )\n",
    "        test_dataset = TitanicDataset(\n",
    "            cls.__create_key,\n",
    "            label_name=label_name,\n",
    "            split_seed=split_seed,\n",
    "            test_size=test_size,\n",
    "            train=False,\n",
    "        )\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        create_key=None,\n",
    "        label_name=\"Survived\",\n",
    "        split_seed=42,\n",
    "        test_size=0.2,\n",
    "        train=True,\n",
    "    ):\n",
    "        # Ensure that the dataset is being constructed properly\n",
    "        if create_key != TitanicDataset.__create_key:\n",
    "            raise ValueError(\n",
    "                \"Illegal initialisation attempt â€” please use create_datasets to initialise.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            data_df = pd.read_csv(\"titanic-dataset.csv\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"Titanic data file not found.\")\n",
    "\n",
    "        # Split the dataset into train and test\n",
    "        x = data_df.drop(columns=[label_name, \"name\", \"ticket\", \"cabin\", \"embarked\", \"boat\", \"body\", \"home.dest\"])\n",
    "        # For the purposes of this coursework, we just impute the missing age and fare with a median value\n",
    "        x[['age']] = x[['age']].fillna(x[['age']].median())\n",
    "        x[['fare']] = x[['fare']].fillna(x[['fare']].median())\n",
    "        y = data_df[label_name]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            x, y, test_size=test_size, random_state=split_seed, shuffle=True\n",
    "        )\n",
    "        if train:\n",
    "            self.raw_data = x_train, y_train\n",
    "        else:\n",
    "            self.raw_data = x_test, y_test\n",
    "\n",
    "        # Preprocess the data\n",
    "        x_train_processed, preprocessor = preprocess_train_data(\n",
    "            x_train, categorical_features=[\"sex\"]\n",
    "        )\n",
    "        x_train = pd.DataFrame(\n",
    "            x_train_processed, columns=preprocessor.get_feature_names_out()\n",
    "        )\n",
    "        x_test_processed = preprocess_test_data(x_test, preprocessor)\n",
    "        x_test = pd.DataFrame(\n",
    "            x_test_processed, columns=preprocessor.get_feature_names_out()\n",
    "        )\n",
    "\n",
    "        # Select data partition and convert to tensors\n",
    "        if train:\n",
    "            samples = x_train\n",
    "            labels = y_train\n",
    "        else:\n",
    "            samples = x_test\n",
    "            labels = y_test\n",
    "        self.samples = torch.tensor(samples.to_numpy(), dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.to_numpy(), dtype=torch.long)\n",
    "        self.features = preprocessor.get_feature_names_out()\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c7c1f-81f5-4806-ad20-358a79df93e6",
   "metadata": {},
   "source": [
    "Finally, we call the code above to load and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae2d1e0-ddab-419b-b741-6d4d6b531bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = TitanicDataset.create_datasets(\n",
    "    test_size=0.2,\n",
    "    split_seed=42,\n",
    ")\n",
    "train_dl = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4283a2e-986b-4554-89e3-3a0c4c152cbb",
   "metadata": {},
   "source": [
    "Note that the invertible transformer allows you to recover the original (unnormalised) feature values, as shown on the example below. You may find this helpful for understanding the produced explanations and commenting on them in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f85249-8c4b-46b0-a856-76c972f524e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.125</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pclass   age sibsp parch   fare   sex\n",
       "0    3.0  35.0   0.0   0.0  7.125  male"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.preprocessor.inverse_transform(test_dataset.samples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441756a7-f828-4764-af69-4eb84b63a975",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93538903-f8cc-4f39-9593-f0ee82096b75",
   "metadata": {},
   "source": [
    "When faced with a new dataset, it is a good practice to perform an exploratory data analysis in order to understand the basic trends in the data. This will also allow you to put the explanations you obtain as part of this coursework into the relevant context. We will use the raw, unnormalised features for this purpose, as they are much more intuitive and human-understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82a5efb-2ce7-4428-a03f-ee0f61f6dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_dataset.raw_data\n",
    "x_train['survived'] = y_train\n",
    "data_df = x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf759d8-e225-4482-a8ba-93d0a604d32e",
   "metadata": {},
   "source": [
    "We start by displaying the feature values and labels for a few samples. The dataset contains data regarding the survival of some of the passengers involved in the [Titanic maritime disaster](https://en.wikipedia.org/wiki/Sinking_of_the_Titanic). The features contained in the data are as follows:\n",
    "* `pclass`: Indicates the travelling class of the given passenger. Note that we treat this feature as numerical, as the different classes introduce a natural order.\n",
    "* `sex`: Indicates the sex of the passenger.\n",
    "* `age`: Provides the age of the passenger.\n",
    "* `sibsp`: Denotes the total number of siblings and spouses of the given passenger also travelling on RMS Titanic.\n",
    "* `parch`: Denotes the total number of parents or children of the given passenger also travelling on RMS Titanic.\n",
    "* `fare`: Indicates the fare paid by the passenger for the journey.\n",
    "* `survived`: The label indicating whether the patient survived the accident (1 = survived, 0 = did not survive).\n",
    "\n",
    "There are other features included in the original dataset (such aspassenger name or point of embarkation), but we choose to ignore them for the purposes of this coursework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bc9b42a-9f57-4502-8d00-9fda6bdb34cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['num__pclass', 'num__age', 'num__sibsp', 'num__parch', 'num__fare',\n",
       "       'cat__sex_female', 'cat__sex_male'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ca8d73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex\n",
      "female    264\n",
      "male      118\n",
      "Name: count, dtype: int64\n",
      "pclass\n",
      "1    152\n",
      "3    140\n",
      "2     90\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print data where data_df['survived'] == 1 and count p_class numbers\n",
    "print(data_df[data_df['survived'] == 1]['sex'].value_counts())\n",
    "print(data_df[data_df['survived'] == 1]['pclass'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d8e9c2-2266-40c2-9d91-3bd25e633eb2",
   "metadata": {},
   "source": [
    "Let us visualise the correlation between the individual columns of the data, computed using the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient). Note that we excluded the `sex` feature from this visualisation, as it is categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "465f7fb2-3f81-4f9a-9ff9-09f45d3a50e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "corr = data_df.drop(columns=[\"sex\"]).corr()\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    xticklabels=corr.columns.values,\n",
    "    yticklabels=corr.columns.values,\n",
    "    annot=True,\n",
    "    fmt='.2g',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba12652-00ee-42f2-b37c-7aabbd7d6b17",
   "metadata": {},
   "source": [
    "Since the previous plot does not include the categorical `sex` column, we also separately visualise its distribution, including the associated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93e255d4-c9d4-4588-aeb3-f563c202b595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df.groupby(['sex', 'survived']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=['#a51900', '#02893b'], xlabel=\"Sex\", ylabel=\"Count\"\n",
    ")\n",
    "plt.legend(['No', 'Yes'], title=\"Survived\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2dc6b6-a871-4509-8049-52d84833c417",
   "metadata": {},
   "source": [
    "**Task 1(a)**: <br />\n",
    "**(i)** Considering the above visualisations, are there any trends or patterns that you can identify in the data? <br />\n",
    "**(ii)** Without having access to any particular model or the associated explanations, which features would you expect to be the most and least important for a neural network trained on the dataset? How can you tell and how certain can you be of your assessment? <br />\n",
    "**(iii)** Apart from inspecting the above plots, is there anything else you could do as part of the exploratory analysis that would allow you to better understand the data and the behaviour of the models trained on it? <br />\n",
    "Please write your answers in a few sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba69c8",
   "metadata": {},
   "source": [
    "**Task 1(a)** ANSWER: <br />\n",
    "**(i)**  \n",
    "\n",
    "`Sex` and `Survival`: From the visualization of `sex` column, the most obvious trend is that women obviously have higher survival rate. \n",
    "\n",
    "`Sex` and `Fare` / `p_class`: From the heat map, the higher the fare, the better the p_class, and thus the better fare or p_class shows the better survival rate. `sex` might correspond to `fare` or `p_class` that more women are in higher class or pay more.\n",
    "<br />\n",
    "\n",
    "**(ii)** \n",
    "According to the heatmap and the bar figure, the most distinguishing features are `sex, fare and p_class`.\n",
    "\n",
    "Sex: Historically, it is known that women were more likely to survive the Titanic disaster due to the \"women and children first\" policy for lifeboats. Thus, even without a correlation coefficient, I would expect the `sex` feature to be highly important in predicting survival.\n",
    "\n",
    "Pclass: There is a notable negative correlation between `pclass` and `survived`. Higher socio-economic status (first class) likely afforded better access to lifeboats. This makes passenger class a potentially strong predictor of survival.\n",
    "\n",
    "Fare: The positive correlation between `fare` and `survived` could indicate that passengers who paid more (likely for better accommodation) had a higher chance of survival. This could be due to the proximity to lifeboats or the prioritization.\n",
    "\n",
    "The least important feature should be `sibsp` and `age` which have the least correlation with `survived`.\n",
    "<br />\n",
    "\n",
    "**(iii)** \n",
    "\n",
    "From common used ways, plots or data visualization are efficient ways to help understande the data. Feature engineering could generate new features from existing data that could better help the model training. Most important, PCA or other dimensionality reduction methods could be applied to identify which feature matters most.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a7e4a2-988d-4580-9fb9-a93f76c3164b",
   "metadata": {},
   "source": [
    "## Model Initialisation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435873c-e279-45c9-b2b0-38faedba32ac",
   "metadata": {},
   "source": [
    "First, we define a global device variable to enable running this code on a GPU or a CPU, as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "65a56bef-3daa-44aa-8e57-d001b3bff2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a96541-7047-4c2b-845f-b823f4a8b239",
   "metadata": {},
   "source": [
    "Here, we define several utility functions for constructing, training and evaluating neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "395dd993-e2e1-46a8-b727-9f646ce4e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torcheval.metrics.functional import binary_f1_score, binary_accuracy, binary_auroc\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def construct_nn(nn_dims, activation_fun):\n",
    "    \"\"\"\n",
    "    Constructs a neural network with the specified architecture.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for i in range(1, len(nn_dims)):\n",
    "        in_dim, out_dim = nn_dims[i-1], nn_dims[i]\n",
    "        layers.append(nn.Linear(in_dim, out_dim))\n",
    "        layers.append(activation_fun())\n",
    "    # Remove the last activation layer and add Sigmoid instead\n",
    "    layers = layers[:-1]\n",
    "    layers.append(nn.Sigmoid())\n",
    "    \n",
    "    return nn.Sequential(*layers).to(DEVICE)\n",
    "\n",
    "def train_nn(model, train_dl, num_epochs=100):\n",
    "    \"\"\"\n",
    "    Trains a neural network using the data from the provided data loader.\n",
    "    \"\"\"\n",
    "    loss_fun = nn.BCELoss()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=0.005)\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in tqdm(range(num_epochs), leave=False):\n",
    "        total_loss = 0\n",
    "        for i, (x, y) in list(enumerate(train_dl)):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = loss_fun(out.squeeze(-1), y.float())\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        losses.append(total_loss)\n",
    "\n",
    "def eval_nn(model, test_dataset):\n",
    "    \"\"\"\n",
    "    Evaluates binary classification performance of a model on the given\n",
    "    test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    loss_fun = nn.BCELoss()\n",
    "    predictions = model(test_dataset.samples.to(DEVICE))\n",
    "    labels = test_dataset.labels.unsqueeze(-1).to(DEVICE)\n",
    "    loss = loss_fun(predictions, labels.float()).item()\n",
    "\n",
    "    predictions = predictions.squeeze(-1).detach()\n",
    "    labels = labels.squeeze(-1).detach()\n",
    "    f1 = binary_f1_score(predictions, labels).item()\n",
    "    accuracy = binary_accuracy(predictions, labels).item()\n",
    "    auc = binary_auroc(predictions, labels).item()\n",
    "\n",
    "    return loss, f1, accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3495eeb-14fa-4ae2-aaaf-b24c64ab8ef9",
   "metadata": {},
   "source": [
    "In this cell, we initialise and train the neural model that we will be explaining in this coursework. For a real-world application, you would typically wish to perform a full hyperparameter search in order to identify the most effective model architecture. However, achieving a maximum performance is not the objective of this coursework, so we just pre-define a model that performs reasonably well on the given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "68254f96-7821-4a53-8615-2edbfee294ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€”â€”â€”â€”â€”â€”â€”[ Model training ]â€”â€”â€”â€”â€”â€”â€”\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d09d6a69e43432eb3a946777451514a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "\n",
      "â€”â€”â€”â€”â€”â€”â€”[ Evaluation ]â€”â€”â€”â€”â€”â€”â€”\n",
      "F1 score: 0.73\n",
      "Accuracy: 0.79\n",
      "AUC: 0.83\n"
     ]
    }
   ],
   "source": [
    "def print_metric(name, value):\n",
    "    print(f\"{name}: {'{:.2f}'.format(round(value, 2))}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "model = construct_nn([7, 256, 256, 1], nn.ReLU).to(DEVICE)\n",
    "\n",
    "print(\"â€”â€”â€”â€”â€”â€”â€”[ Model training ]â€”â€”â€”â€”â€”â€”â€”\")\n",
    "train_nn(model, train_dl, num_epochs=1000)\n",
    "print(\"Training completed!\")\n",
    "print()\n",
    "\n",
    "print(\"â€”â€”â€”â€”â€”â€”â€”[ Evaluation ]â€”â€”â€”â€”â€”â€”â€”\")\n",
    "test_loss, f1, accuracy, auc = eval_nn(model, test_dataset)\n",
    "print_metric(\"F1 score\", f1)\n",
    "print_metric(\"Accuracy\", accuracy)\n",
    "print_metric(\"AUC\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6034c1b-500d-4e02-a6e7-742d62208c8e",
   "metadata": {},
   "source": [
    "## Feature Attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fa2e83-1342-4a8c-b1e7-d1188455eeb6",
   "metadata": {},
   "source": [
    "In this section of the coursework, you will implement SHAP as introduced in the lectures and conduct additional experiments with various feature attribution methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e7328-82bd-41aa-bcb9-6488e575c3f4",
   "metadata": {},
   "source": [
    "### SHAP Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf73aa-f5e3-4c36-9875-19e38c5d5c99",
   "metadata": {},
   "source": [
    "**Task 2(a)(i)**: As a first step in implementing SHAP, define a `compute_coefficient` function to compute the SHAP coalition coefficient/weight as specified by the formula from the lectures:\n",
    "\n",
    "$$g_{SHAP}(\\mathcal{M},\\mathbf{x},i) = \\sum_{\\mathbf{z} \\subseteq \\mathbf{x}}{\\frac{|\\mathbf{z}|!(n - |\\mathbf{z}| - 1)! }{n!} \\mathcal{M}( \\mathbf{z}) - \\mathcal{M}(\\mathbf{z}_{-i})} \\nonumber$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "564deb60-0275-4b75-a2a8-b2e61ef4563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_coefficient(num_in_coalition, total_features):\n",
    "    \"\"\"\n",
    "    Computes the SHAP coefficient for a coalition.\n",
    "\n",
    "    Parameters:\n",
    "        num_in_coalition (int): The number of features in the given coalition\n",
    "        total_features (int): The total number of considered features\n",
    "\n",
    "    Returns:\n",
    "        coefficient (float): The SHAP weight for the given coalition\n",
    "    \"\"\"\n",
    "    numerator = math.factorial(num_in_coalition) * math.factorial(total_features - num_in_coalition - 1)\n",
    "    denominator = math.factorial(total_features)\n",
    "    \n",
    "    # Calculate the SHAP coefficient\n",
    "    coefficient = numerator / denominator\n",
    "    return coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8131b77-5eac-471c-a1a8-ff138f4b4541",
   "metadata": {},
   "source": [
    "**Task 2(a)(ii)**: Next, define a function `generate_coalitions`, which will return the list representing all the possible coalitions for a possible feature.\n",
    "\n",
    "Hint #1: You may find it helpful to use [itertools](https://docs.python.org/3/library/itertools.html) and [Python generators](https://wiki.python.org/moin/Generators) for implementing this function.\n",
    "\n",
    "Hint #2: Passing a full list of feature IDs is not strictly necessary here, but you will find this list helpful for implementing other functions, so we also recommend taking it as a parameter here. As an example, for the Titanic dataset, this list could look like `[0, 1, 2, 3, 4, 5, 5]` (note the repeated `5` for the one-hot-encoded `sex` feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3e60209-d01b-4947-bba5-e13b5385d43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(set(), {0, 1, 3}), ({0}, {1, 3}), ({1}, {0, 3}), ({3}, {0, 1}), ({0, 1}, {3}), ({0, 3}, {1}), ({1, 3}, {0}), ({0, 1, 3}, set())]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_coalitions(feature_ids, target_feature_id):\n",
    "    \"\"\"\n",
    "    Generates the possible feature coalitions for the purpose of computing the Shapley value\n",
    "    for the target feature.\n",
    "\n",
    "    Parameters:\n",
    "        feature_ids (list): A list with feature IDs from 0 to N (where N is\n",
    "            the total number of features) identifying the used features. Distinct\n",
    "            columns for one-hot-encoded features should be assigned the same\n",
    "            numerical ID.\n",
    "        target_feature_id (int): The ID of the removed feature for which the coalitions\n",
    "            should be generated.\n",
    "\n",
    "    Retruns:\n",
    "        coalitions (list): A nested list structure of coalitions in the form:\n",
    "            [(set(coalition 1 in features set), set(coalition 1 out features set)), ...].\n",
    "            Note that feature_id should not appear in either of the in/out lists.\n",
    "    \"\"\"\n",
    "    # Remove the target feature ID from the list of feature IDs\n",
    "    feature_ids = [f for f in feature_ids if f != target_feature_id]\n",
    "\n",
    "    # Generate all possible coalitions without the target feature\n",
    "    coalitions = []\n",
    "    for L in range(0, len(feature_ids)+1):\n",
    "        for subset in itertools.combinations(feature_ids, L):\n",
    "            # The current subset is the coalition with the feature\n",
    "            # The set difference between all features and the current subset\n",
    "            # is the coalition without the feature\n",
    "            in_coalition = set(subset)\n",
    "            out_coalition = set(feature_ids) - in_coalition\n",
    "            coalitions.append((in_coalition, out_coalition))\n",
    "            \n",
    "    return coalitions\n",
    "\n",
    "coalitions = generate_coalitions([0, 1, 2, 3], 2)\n",
    "print(coalitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6fe694-5010-47c6-a3dc-68ffd032eb1b",
   "metadata": {},
   "source": [
    "**Task 2(a)(iii)**: Next, implement a function `delete_features` that deletes the specified features from the given input tensor `x`. In contrast with the setting in the SHAP tutorial, the majority of features considered in this coursework are non-binary, which makes the deletion of features slightly more challenging. The general procedure for performing the deletion can be described as follows:\n",
    "1. For each sample in `x` and each deleted feature, randomly sample the value of the deleted feature from another data point in the background dataset\n",
    "2. If the sampled value is identical to the current value of the deleted feature, continue sampling new values until finding one that differs. This ensures that feature deletion actually changes the values of categorical variables or variables with few possible values.\n",
    "3. Replace the value of the deleted feature in the currently considered sample with the newly sampled value\n",
    "\n",
    "Hint #1: Boolean tensor masks \"selecting\" certain features can be very helpful here.\n",
    "\n",
    "Hint #2: Make sure not to overwrite values in the original `x` when deleting features. Instead, the function should return a new tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a75928ee-3f2b-4427-8f41-45cfbe239eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_features(x, background_dataset, feature_ids, deleted_feature_ids):\n",
    "    \"\"\"\n",
    "    Deletes the specified features from inputs x using the background dataset.\n",
    "\n",
    "    Parameters:\n",
    "        x (Tensor): A tensor of inputs with the shape (batch_size, num_features).\n",
    "        background_dataset (Tensor): A tensor of background data samples with the same shape as x.\n",
    "        feature_ids (list): A list with feature IDs, same as in generate_coalitions.\n",
    "        deleted_feature_ids (set): A set with feature IDs to be deleted from x.\n",
    "\n",
    "    Returns:\n",
    "        x_deleted (tensor): A new tensor of inputs with the specified features deleted.\n",
    "    \"\"\"    \n",
    "    # Make the sampling deterministic\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "        # Initialize x_deleted as a copy of x to avoid modifying the original tensor\n",
    "    x_deleted = x.clone().detach()\n",
    "    \n",
    "    # Loop over each feature to be deleted\n",
    "    for feature_id in deleted_feature_ids:\n",
    "        # Check if feature_id is in the list of feature IDs\n",
    "        if feature_id in feature_ids:\n",
    "            # Get the index of the feature_id in the list of feature_ids\n",
    "            feature_index = feature_ids.index(feature_id)\n",
    "            \n",
    "            # Loop over each sample in x\n",
    "            for i in range(x.size(0)):\n",
    "                # Sample a value for the deleted feature from the background dataset\n",
    "                # Keep sampling until a different value is found\n",
    "                while True:\n",
    "                    sample_index = np.random.choice(background_dataset.size(0))\n",
    "                    sampled_value = background_dataset[sample_index, feature_index]\n",
    "                    \n",
    "                    if sampled_value != x[i, feature_index]:\n",
    "                        break\n",
    "                \n",
    "                # Replace the value of the deleted feature with the new value\n",
    "                x_deleted[i, feature_index] = sampled_value\n",
    "\n",
    "    return x_deleted\n",
    "# x = torch.rand((4, 5))  # Example input tensor\n",
    "# background_dataset = torch.rand((10, 5))  # Example background dataset\n",
    "# feature_ids = [0, 1, 2, 3, 4]\n",
    "# deleted_feature_ids = {1, 3}\n",
    "# x_deleted = delete_features(x, background_dataset, feature_ids, deleted_feature_ids)\n",
    "# print(x, background_dataset, x_deleted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679f0dc8-1542-4989-b148-41d5e5889e5b",
   "metadata": {},
   "source": [
    "**Task 2(a)(iv)**: Finally, put everything together in the `shap_attribute` function, which will compute the SHAP attributions for the given input and model. Note that the function also takes in a `target_idx` specifying for which output neuron the explanations should be computed. This is not strictly necessary for the Titanic model, which only has a single Sigmoid output, but will be needed once you start working with a more complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aecaa2ba-f48f-474e-9503-d22598e554e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import embed\n",
    "def shap_attribute(model, x, background_dataset, feature_ids, target_idx=0):\n",
    "    \"\"\"\n",
    "    Computes the SHAP attributions for the given input and model.\n",
    "\n",
    "    Parameters:\n",
    "        model (Object): A PyTorch model for which the attributions should be computed.\n",
    "        x (Tensor): Inputs for which the explanations should be computed, in shape (batch_size, num_features).\n",
    "        background_dataset (Tensor): A tensor of background data samples with the same shape as x.\n",
    "        feature_ids (list): A list with feature IDs, same as in generate_coalitions.\n",
    "        target_idx (int): The ID of the target neuron for which to compute an explanation\n",
    "            (useful for classification tasks with multiple labels where it should correspond\n",
    "\n",
    "    Returns:\n",
    "        attributions (Tensor): A tensor of SHAP attributions, with the same shape as the input\n",
    "    \"\"\"\n",
    "    # Make sure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize a tensor to hold the SHAP values\n",
    "    shap_values = torch.zeros_like(x)\n",
    "    \n",
    "    # Compute the number of features\n",
    "    num_features = len(feature_ids)\n",
    "    \n",
    "    # Iterate over all features to compute SHAP values\n",
    "    for idx, i in enumerate(feature_ids):\n",
    "        # Generate coalitions excluding the current feature i\n",
    "        coalitions = generate_coalitions(feature_ids, i)\n",
    "        \n",
    "        # Iterate over all coalitions\n",
    "        for in_coalition, out_coalition in coalitions:\n",
    "            # Compute the SHAP coalition coefficient\n",
    "            coeff = compute_coefficient(len(in_coalition), num_features)\n",
    "            \n",
    "            # Create a copy of x with feature i included\n",
    "            x_in = delete_features(x, background_dataset, feature_ids, out_coalition)\n",
    "            \n",
    "            # Create a copy of x with feature i excluded\n",
    "            x_out = delete_features(x, background_dataset, feature_ids, in_coalition.union({i}))\n",
    "            \n",
    "            # Compute the model's output for both modified inputs\n",
    "            pred_in = model(x_in)[:, target_idx]\n",
    "            pred_out = model(x_out)[:, target_idx]\n",
    "            \n",
    "            # Update shap_values for each sample independently\n",
    "            shap_values[:, idx] += (pred_in - pred_out) * coeff\n",
    "    \n",
    "    return shap_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3017834b-bf1c-48e4-ab1c-ff0d26ccab5c",
   "metadata": {},
   "source": [
    "### Additional Explanation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33325644-8c95-4e9f-aa94-9cf9206d2268",
   "metadata": {},
   "source": [
    "Apart from SHAP, which you just implemented, you will also be experimenting with two more feature attribution methods implemented in the [Captum](https://captum.ai/) library â€” [Shapley Value Sampling](https://captum.ai/api/shapley_value_sampling.html) and [DeepLIFT](https://captum.ai/api/deep_lift.html). Shapley Value Sampling is a more computationally tractable approximation of SHAP and computes the scores by randomly sampling a fixed number of coalitions instead of considering all of them. Meanwhile, DeepLIFT is a fast gradient-based attribution method specifically designed for neural models. If you are interested, you can learn more about DeepLIFT in [its original paper](https://arxiv.org/abs/1704.02685).\n",
    "\n",
    "To get you started, we provide an example of how to use the Captum library to generate Shapley Value Sampling attributions for the first sample from the Titanic test set (note that the library also allows you to compute attributions for a batch of inputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "974eca85-1658-4222-95fc-bf3d180baeb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.2674,  0.0000,  0.0000,  0.1867, -0.5861, -0.5861]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from captum.attr import ShapleyValueSampling\n",
    "\n",
    "# Note that this is similar to feature_ids from the implementation above,\n",
    "# but the shape and the data type are different\n",
    "feature_mask = torch.tensor([[0, 1, 2, 3, 4, 5, 5]]).to(DEVICE)\n",
    "svs = ShapleyValueSampling(model)\n",
    "attributions = svs.attribute(test_dataset.samples[[0]].to(DEVICE), target=-1, feature_mask=feature_mask)\n",
    "attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e108b2-15d8-4c0b-bf6f-a64a4fe3b9e2",
   "metadata": {},
   "source": [
    "Notice that, in contrast with the SHAP implementation above, we did not need to pass in the background dataset. This is because Captum takes a slightly different approach to deleting features and instead replaces them with a pre-specified baseline value (see the `baselines` parameter description in the [documentation](https://captum.ai/api/shapley_value_sampling.html)). For the purposes of this coursework, it is fine to use the default (zero) baseline for both Shapley Value Sampling and DeepLIFT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c24e08-7871-44fe-afdd-d15c3c8d1fee",
   "metadata": {},
   "source": [
    "### Feature Attribution Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d4669-00e6-43bb-826f-c0f12aa45f96",
   "metadata": {},
   "source": [
    "In this section, you will conduct several experiments associated with feature attribution methods.\n",
    "\n",
    "**Task 2(b)**: Using your implementation of SHAP and Captum implementations of Shapley Value Sampling and DeepLIFT, compute feature attributions for 10 randomly selected instances from the Titanic test set. Then answer the following questions: <br />\n",
    "**(i)** Which features generally seem to be the most important and least important for the explained model according to each of the explanations? <br />\n",
    "**(ii)** Are there any substantial differences between the different attribution methods? What might be the possible reasons for the different methods returning different attribution scores? <br />\n",
    "**(iii)** Do the attribution scores match your expectations for the most/least important features from task 1(a)(ii)? What might be the reasons for a user's expected explanations differing from the computed attribution explanations? <br />\n",
    "**(iv)** Considering the insights gained from the exploratory data analysis and the feature attribution explanations, as well as the definitions of the explanations themselves, what are the potential advantages/disadvantages of each of these methods when trying to understand the behaviour of a model on a particular dataset? <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c2680c1c-9276-45c1-94ae-c10a660b74f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_list: ['num__pclass' 'num__age' 'num__sibsp' 'num__parch' 'num__fare'\n",
      " 'cat__sex_female' 'cat__sex_male']\n"
     ]
    }
   ],
   "source": [
    "from captum.attr import ShapleyValueSampling, DeepLift\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"feature_list:\", train_dataset.features)\n",
    "select_indices = torch.randint(0, len(test_dataset), (10,))\n",
    "feature_mask = torch.tensor([[0, 1, 2, 3, 4, 5, 5]]).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "62459a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVS: tensor([ 0.1471,  0.1187,  0.0073,  0.0298,  0.1627, -0.2436, -0.2436])\n",
      "DeepLift: tensor([ 0.0337,  0.2348, -0.0061,  0.0742,  0.0507, -0.1114, -0.0539],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "Custom SHAP: tensor([ 0.1453, -0.1519, -0.0388, -0.0388, -0.0580, -0.0880, -0.0880],\n",
      "       grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "svs = ShapleyValueSampling(model)\n",
    "svs_attributions = svs.attribute(test_dataset.samples[select_indices].to(DEVICE), target=0, feature_mask=feature_mask)\n",
    "print(\"SVS:\", svs_attributions.mean(dim=0))\n",
    "\n",
    "deeplift = DeepLift(model)\n",
    "deeplift_attributions = deeplift.attribute(test_dataset.samples[select_indices].to(DEVICE), target=0)\n",
    "print(\"DeepLift:\", deeplift_attributions.mean(dim=0))\n",
    "\n",
    "shap_values = shap_attribute(model, test_dataset.samples[select_indices], train_dataset.samples, [0, 1, 2, 3, 4, 5, 5])\n",
    "print(\"Custom SHAP:\", shap_values.mean(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aef73a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the attributions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "x = train_dataset.features\n",
    "y1 = svs_attributions.mean(dim=0).cpu().detach().numpy()\n",
    "y2 = deeplift_attributions.mean(dim=0).cpu().detach().numpy()\n",
    "y3 = shap_values.mean(dim=0).cpu().detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y1,'-o', label=\"SVS\")\n",
    "ax.plot(x, y2,'-o', label=\"DeepLift\")\n",
    "ax.plot(x, y3, '-o', label=\"Custom SHAP\")\n",
    "ax.set_ylabel(\"Attribution\")\n",
    "ax.set_xlabel(\"Feature\")\n",
    "ax.set_title(\"Attributions\")\n",
    "ax.legend()\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f1001-218e-46a8-b4cf-303f2939c234",
   "metadata": {},
   "source": [
    "**Task 2(c)**: Perform a quantitative evaluation of the different attribution methods by computing their mean [infidelity](https://captum.ai/api/metrics.html) on the full Titanic dataset. On a high level, infidelity aims to estimate how closely the generated explanations correspond with the behaviour of the explained model by slightly perturbing the inputs and measuring how much the observed change in the model output differs from the change predicted by the corresponding feature attributions (when considering a linear model with the same weights as the feature attribution scores). If you are interested, you can find more details regarding this metric in [the original paper](https://arxiv.org/abs/1901.09392). A downside of the infidelity metric is that one needs to define a suitable perturbation function for changing the model inputs, which can significantly affect the results. In this coursework, we provide you with a perturbation function adding Gaussian noise to continuous features and performing resampling for categorical features. In your evaluation, you should experiment with two or three different standard deviations and categorical resampling probabilities. Once you are done, add a table summarising the results to your report and comment on the findings. Note that lower infidelity scores are better.\n",
    "\n",
    "Note: You should use `normalize=True` and `n_perturb_samples=10` as parameters to the Captum's infidelity function and set the same Torch and NumPy seeds before computing the infidelity for each method (so that all methods are evaluated using the same sample perturbations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9ffeb0f4-180f-4c7c-8a3f-258d3725ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.metrics import infidelity, infidelity_perturb_func_decorator\n",
    "\n",
    "def perturb_func_constructor(noise_scale, cat_resample_proba, background_dataset, feature_ids, n_perturb_samples=10):\n",
    "    \"\"\"\n",
    "    You can call this function to construct a perturbation function with the desired parameters,\n",
    "    which can then be provided as the perturb_func parameter to the infidelity metric implementation\n",
    "    from Captum.\n",
    "\n",
    "     Parameters:\n",
    "        noise_scale (float): A standard deviation of the Gaussian noise added to the continuous features.\n",
    "        cat_resample_proba (float): Probability of resampling a categorical feature.\n",
    "        background_dataset (Tensor): A tensor of background data samples with the shape (num_samples, num_features).\n",
    "        feature_ids (list): A list with feature IDs, same as in generate_coalitions.\n",
    "        n_perturb_samples (int): The number of perturbed samples for each input. Should match the value\n",
    "            of the corresponding parameter to the Captum's infidelity function.\n",
    "\n",
    "    Returns:\n",
    "        perturb_func (function): A perturbation function compatible with Captum\n",
    "    \"\"\"\n",
    "    @infidelity_perturb_func_decorator(True)\n",
    "    def perturb_func(inputs):        \n",
    "        # Construct masks for noise and resampling categorical variables\n",
    "        noise_mask = torch.ones(1, inputs.size(1)).to(DEVICE)\n",
    "        # We assume that categorical features are one-hot-encoded\n",
    "        i = 0\n",
    "        current_span_start = 0\n",
    "        categorical_spans = []\n",
    "        while i < len(feature_ids) - 1:    \n",
    "            if feature_ids[i] != feature_ids[i + 1] and current_span_start != i:\n",
    "                categorical_spans.append((current_span_start, i))\n",
    "                current_span_start = i + 1\n",
    "            elif feature_ids[i] != feature_ids[i + 1]:\n",
    "                current_span_start = i + 1\n",
    "            elif feature_ids[i] == feature_ids[i + 1] and i == len(feature_ids) - 2:\n",
    "                categorical_spans.append((current_span_start, i + 1))\n",
    "            i += 1\n",
    "                \n",
    "        cat_resample_masks = []\n",
    "        for i, (s, e) in enumerate(categorical_spans):\n",
    "            cat_resample_mask = torch.zeros(inputs.shape).to(DEVICE)\n",
    "            probabilities = torch.full((inputs.size(0), 1), cat_resample_proba)\n",
    "            resample_tensor = torch.bernoulli(probabilities)\n",
    "            noise_mask[:, s:e] = 0.\n",
    "            cat_resample_mask[:, s:e] = resample_tensor\n",
    "            cat_resample_masks.append(cat_resample_mask)\n",
    "\n",
    "        # Add noise to continuous features only\n",
    "        noise = torch.tensor(np.random.normal(0, noise_scale, inputs.shape)).float().to(DEVICE) * noise_mask\n",
    "        perturbed_inputs = inputs - noise\n",
    "\n",
    "        # Randomly resample categorical variables\n",
    "        if categorical_spans:\n",
    "            expanded_background_dataset = background_dataset.repeat((n_perturb_samples, 1))\n",
    "            for cat_resample_mask in cat_resample_masks:\n",
    "                random_perm = torch.randperm(expanded_background_dataset.size(0))\n",
    "                random_samples = expanded_background_dataset[random_perm[:inputs.size(0)]]\n",
    "                perturbed_inputs = perturbed_inputs * (1 - cat_resample_mask) + random_samples * cat_resample_mask\n",
    "\n",
    "        return perturbed_inputs\n",
    "\n",
    "    return perturb_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a36cb06b-f3ae-4888-905f-54d8cb5ae7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_resample_proba: 0.1\n",
      "Mean Infidelity for Attribution Method SVS: tensor([4.8187e-04, 2.7669e-05, 1.1149e-01, 1.6079e-15, 7.3970e-04, 1.7769e-01,\n",
      "        3.8885e-03, 1.9438e-01, 3.0821e-03, 6.8915e-02])\n",
      "Mean Infidelity for Attribution Method DeepLift: tensor([2.2665e-03, 7.6320e-09, 1.3202e-01, 4.5511e-20, 1.4984e-06, 1.0051e-01,\n",
      "        7.2248e-02, 1.6410e-01, 6.6933e-08, 3.0955e-02])\n",
      "Mean Infidelity for Attribution Method SHAP: tensor([9.1681e-04, 3.2470e-06, 1.2008e-01, 3.5033e-19, 4.7857e-02, 2.4039e-01,\n",
      "        2.3324e-02, 2.9830e-01, 1.2785e-06, 8.8509e-02])\n",
      "cat_resample_proba: 0.3\n",
      "Mean Infidelity for Attribution Method SVS: tensor([8.2716e-02, 3.4099e-02, 1.9186e-01, 1.0642e-14, 5.2943e-02, 2.8751e-01,\n",
      "        1.4194e-02, 4.8567e-03, 4.8773e-06, 6.6129e-02])\n",
      "Mean Infidelity for Attribution Method DeepLift: tensor([8.1518e-02, 1.6101e-02, 1.2935e-01, 9.0962e-20, 1.2174e-03, 1.9450e-01,\n",
      "        3.7912e-02, 1.9734e-01, 1.1036e-06, 9.9509e-03])\n",
      "Mean Infidelity for Attribution Method SHAP: tensor([8.6134e-02, 7.9670e-07, 1.7455e-01, 5.1611e-18, 4.7039e-03, 3.9522e-01,\n",
      "        4.1060e-02, 1.2065e-01, 4.8797e-04, 8.3407e-03])\n",
      "cat_resample_proba: 0.5\n",
      "Mean Infidelity for Attribution Method SVS: tensor([3.8659e-02, 1.0016e-02, 1.1947e-01, 7.8669e-17, 6.6978e-02, 2.6784e-01,\n",
      "        2.3966e-02, 4.3682e-02, 1.5148e-05, 1.7900e-01])\n",
      "Mean Infidelity for Attribution Method DeepLift: tensor([7.9994e-02, 1.5143e-01, 8.4096e-02, 1.5390e-11, 6.0269e-06, 1.4896e-01,\n",
      "        1.7328e-02, 2.6370e-01, 3.4231e-02, 2.1240e-02])\n",
      "Mean Infidelity for Attribution Method SHAP: tensor([1.6878e-01, 2.6039e-10, 1.4993e-01, 1.9919e-11, 1.0158e-02, 5.4988e-01,\n",
      "        2.4385e-02, 1.6531e-01, 3.1662e-04, 1.6303e-02])\n",
      "cat_resample_proba: 0.7\n",
      "Mean Infidelity for Attribution Method SVS: tensor([8.8284e-02, 7.4271e-02, 1.2431e-01, 2.7998e-20, 2.2897e-08, 5.7972e-01,\n",
      "        6.2366e-02, 2.9796e-01, 9.4060e-06, 1.0800e-01])\n",
      "Mean Infidelity for Attribution Method DeepLift: tensor([7.8174e-02, 3.0856e-01, 1.1226e-01, 1.6920e-12, 3.1376e-02, 6.9906e-01,\n",
      "        2.1704e-02, 2.1459e-01, 7.7289e-07, 3.8803e-03])\n",
      "Mean Infidelity for Attribution Method SHAP: tensor([7.8946e-02, 1.5011e-01, 2.2298e-01, 5.5289e-21, 3.2882e-03, 3.6080e-01,\n",
      "        6.3336e-02, 5.8393e-02, 1.4080e-04, 4.3396e-02])\n",
      "cat_resample_proba: 0.9\n",
      "Mean Infidelity for Attribution Method SVS: tensor([4.5289e-02, 4.3015e-02, 1.5855e-01, 9.4195e-18, 2.9262e-03, 1.1382e-01,\n",
      "        2.3645e-02, 1.1884e-01, 1.4176e-06, 8.9901e-02])\n",
      "Mean Infidelity for Attribution Method DeepLift: tensor([1.6499e-01, 2.6659e-01, 1.4944e-01, 7.3672e-15, 3.3298e-02, 7.1058e-01,\n",
      "        2.9322e-02, 2.4295e-02, 1.0028e-07, 2.9012e-02])\n",
      "Mean Infidelity for Attribution Method SHAP: tensor([1.8769e-01, 6.3608e-02, 1.1789e-01, 1.5588e-16, 2.9595e-02, 1.7200e-01,\n",
      "        1.2999e-02, 1.9742e-01, 1.1762e-08, 6.8635e-02])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from captum.metrics import infidelity\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "noise_scale = 0.1\n",
    "cat_resample_probas = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "background_dataset = train_dataset.samples\n",
    "feature_ids = [0,1,2,3,4,5,5]\n",
    "\n",
    "for cat_resample_proba in cat_resample_probas:\n",
    "    perturb_func = perturb_func_constructor(noise_scale, cat_resample_proba, background_dataset, feature_ids)\n",
    "    infidelity_1 = infidelity(model, perturb_func, test_dataset.samples[select_indices].to(DEVICE), svs_attributions,  normalize=True, n_perturb_samples=10)\n",
    "    infidelity_2 = infidelity(model, perturb_func, test_dataset.samples[select_indices].to(DEVICE),deeplift_attributions,  normalize=True, n_perturb_samples=10)\n",
    "    infidelity_3 = infidelity(model, perturb_func, test_dataset.samples[select_indices].to(DEVICE),shap_values,  normalize=True, n_perturb_samples=10)\n",
    "    print(\"cat_resample_proba:\", cat_resample_proba)\n",
    "    print(\"Mean Infidelity for Attribution Method SVS:\", infidelity_1)\n",
    "    print(\"Mean Infidelity for Attribution Method DeepLift:\", infidelity_2)\n",
    "    print(\"Mean Infidelity for Attribution Method SHAP:\", infidelity_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611be864-83ac-467c-8925-e5dfa6927463",
   "metadata": {},
   "source": [
    "**Task 2(d)**: Evaluate the computational efficiency of the different methods by taking the following steps: <br />\n",
    "**(i)** Preproccess the [Dry Bean Dataset](https://archive.ics.uci.edu/dataset/602/dry+bean+dataset), similarly to what we have done for Titanic. You can find the description of the different features on the dataset webpage along with the instructions on how to import the data in a Python environment. You do not need to perform any exploratory data analysis for this dataset. <br />\n",
    "**(ii)** Train an additional neural model on the preprocessed data. Briefly report the key performance metrics for the model in your report. <br />\n",
    "**(iii)** Compute the runtimes required to produce the attribution scores for the different methods when considering the first 200 samples in the Titanic and Dry Bean test sets. Report the results in a table in your report. Which methods seem to be the most/least computationally efficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d411bc2f-3ece-4352-8e2f-66c771e5916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "class DryBeanDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the Titanic dataset.\n",
    "    \"\"\"\n",
    "    __create_key = object()\n",
    "\n",
    "    @classmethod\n",
    "    def create_datasets(\n",
    "        cls,\n",
    "        label_name=\"Class\",\n",
    "        split_seed=42,\n",
    "        test_size=0.2,\n",
    "    ):\n",
    "        train_dataset = DryBeanDataset(\n",
    "            cls.__create_key,\n",
    "            label_name=label_name,\n",
    "            split_seed=split_seed,\n",
    "            test_size=test_size,\n",
    "            train=True,\n",
    "        )\n",
    "        test_dataset = DryBeanDataset(\n",
    "            cls.__create_key,\n",
    "            label_name=label_name,\n",
    "            split_seed=split_seed,\n",
    "            test_size=test_size,\n",
    "            train=False,\n",
    "        )\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        create_key=None,\n",
    "        label_name=\"Class\",\n",
    "        split_seed=42,\n",
    "        test_size=0.2,\n",
    "        train=True,\n",
    "    ):\n",
    "        label_encoder = LabelEncoder()\n",
    "\n",
    "        # Ensure that the dataset is being constructed properly\n",
    "        if create_key != DryBeanDataset.__create_key:\n",
    "            raise ValueError(\n",
    "                \"Illegal initialisation attempt â€” please use create_datasets to initialise.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            data_df = pd.read_excel(\"Dry_Bean_Dataset.xlsx\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"DryBeanDataset data file not found.\")\n",
    "\n",
    "        # Split the dataset into train and test\n",
    "        x = data_df.drop(columns=[label_name])\n",
    "\n",
    "        y = data_df[label_name]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            x, y, test_size=test_size, random_state=split_seed, shuffle=True\n",
    "        )\n",
    "        label_encoder.fit(y_train)  # Fit label encoder on training set labels\n",
    "        self.label_encoder = label_encoder\n",
    "        if train:\n",
    "            self.raw_data = x_train, y_train\n",
    "        else:\n",
    "            self.raw_data = x_test, y_test\n",
    "\n",
    "        # Preprocess the data\n",
    "        x_train_processed, preprocessor = preprocess_train_data(\n",
    "            x_train\n",
    "        )\n",
    "        x_train = pd.DataFrame(\n",
    "            x_train_processed, columns=preprocessor.get_feature_names_out()\n",
    "        )\n",
    "        x_test_processed = preprocess_test_data(x_test, preprocessor)\n",
    "        x_test = pd.DataFrame(\n",
    "            x_test_processed, columns=preprocessor.get_feature_names_out()\n",
    "        )\n",
    "\n",
    "        # Select data partition and convert to tensors\n",
    "        if train:\n",
    "            samples = x_train\n",
    "            labels = torch.tensor(label_encoder.transform(y_train), dtype=torch.long)\n",
    "\n",
    "        else:\n",
    "            samples = x_test\n",
    "            labels = torch.tensor(label_encoder.transform(y_test), dtype=torch.long)\n",
    "\n",
    "        self.samples = torch.tensor(samples.to_numpy(), dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.features = preprocessor.get_feature_names_out()\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5719c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_DB, test_dataset_DB = DryBeanDataset.create_datasets(\n",
    "    test_size=0.2,\n",
    "    split_seed=42,\n",
    ")\n",
    "train_dl_DB = DataLoader(\n",
    "    dataset=train_dataset_DB,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_dl_DB = DataLoader(\n",
    "    dataset=test_dataset_DB,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8c8b8de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11073    DERMASON\n",
       "13172    DERMASON\n",
       "11587    DERMASON\n",
       "12492    DERMASON\n",
       "430         SEKER\n",
       "           ...   \n",
       "5191         CALI\n",
       "13418    DERMASON\n",
       "5390         CALI\n",
       "860         SEKER\n",
       "7270        HOROZ\n",
       "Name: Class, Length: 10888, dtype: object"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_db, y_train_db = train_dataset_DB.raw_data\n",
    "x_train_db['class'] = y_train_db\n",
    "data_df_db = x_train_db\n",
    "y_train_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b7a7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e6a24131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torcheval.metrics.functional import multiclass_f1_score, multiclass_accuracy, multiclass_auroc\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def construct_nn_db(nn_dims, activation_fun):\n",
    "    \"\"\"\n",
    "    Constructs a neural network with the specified architecture.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for i in range(1, len(nn_dims)):\n",
    "        in_dim, out_dim = nn_dims[i-1], nn_dims[i]\n",
    "        layers.append(nn.Linear(in_dim, out_dim))\n",
    "        layers.append(nn.BatchNorm1d(out_dim))\n",
    "        layers.append(activation_fun())\n",
    "    # Remove the last activation layer and add Sigmoid instead\n",
    "    layers = layers[:-1]\n",
    "    layers.append(nn.Sigmoid())\n",
    "    \n",
    "    return nn.Sequential(*layers).to(DEVICE)\n",
    "\n",
    "def train_nn_db(model, train_dl, num_epochs=100):\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=0.05)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\", leave=False):\n",
    "        total_loss = 0\n",
    "        for x, y in train_dl:\n",
    "            opt.zero_grad()\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            out = model(x)\n",
    "            loss = loss_fun(out.squeeze(0), y)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        \n",
    "\n",
    "def eval_db(model, test_dataset):\n",
    "    model.eval()\n",
    "\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    # Assuming test_dataset is a DataLoader\n",
    "    for x, y in test_dataset:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            output = model(x)\n",
    "            predictions.append(output)\n",
    "            labels.append(y)\n",
    "    \n",
    "    predictions = torch.cat(predictions).cpu()\n",
    "    labels = torch.cat(labels).cpu()\n",
    "\n",
    "    loss = loss_fun(predictions, labels).item()\n",
    "\n",
    "    # Compute metrics\n",
    "    f1 = multiclass_f1_score(predictions, labels, num_classes=7).item()\n",
    "    accuracy = multiclass_accuracy(predictions, labels).item()\n",
    "    auc = multiclass_auroc(predictions.softmax(dim=1), labels, num_classes=7).item()\n",
    "\n",
    "    return loss, f1, accuracy, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "81748d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€”â€”â€”â€”â€”â€”â€”[ Evaluation ]â€”â€”â€”â€”â€”â€”â€”\n",
      "F1 score: 0.04\n",
      "Accuracy: 0.04\n",
      "AUC: 0.50\n",
      "â€”â€”â€”â€”â€”â€”â€”[ Model training ]â€”â€”â€”â€”â€”â€”â€”\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d715c00b884a4e92c66c144e34b59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "\n",
      "â€”â€”â€”â€”â€”â€”â€”[ Evaluation ]â€”â€”â€”â€”â€”â€”â€”\n",
      "F1 score: 0.25\n",
      "Accuracy: 0.25\n",
      "AUC: 0.50\n"
     ]
    }
   ],
   "source": [
    "def print_metric(name, value):\n",
    "    print(f\"{name}: {'{:.2f}'.format(round(value, 2))}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "model_db = construct_nn([16, 256, 7], nn.ReLU).to(DEVICE)\n",
    "print(\"â€”â€”â€”â€”â€”â€”â€”[ Evaluation ]â€”â€”â€”â€”â€”â€”â€”\")\n",
    "test_loss, f1, accuracy, auc = eval_db(model_db, test_dl_DB)\n",
    "print_metric(\"F1 score\", f1)\n",
    "print_metric(\"Accuracy\", accuracy)\n",
    "print_metric(\"AUC\", auc)\n",
    "print(\"â€”â€”â€”â€”â€”â€”â€”[ Model training ]â€”â€”â€”â€”â€”â€”â€”\")\n",
    "train_nn_db(model_db, train_dl_DB, num_epochs=100)\n",
    "print(\"Training completed!\")\n",
    "print()\n",
    "print(\"â€”â€”â€”â€”â€”â€”â€”[ Evaluation ]â€”â€”â€”â€”â€”â€”â€”\")\n",
    "test_loss, f1, accuracy, auc = eval_db(model_db, test_dl_DB)\n",
    "print_metric(\"F1 score\", f1)\n",
    "print_metric(\"Accuracy\", accuracy)\n",
    "print_metric(\"AUC\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "02ea06fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.1305510242317998\n",
      "Validation Loss: 1.078180892522945\n",
      "Epoch 2, Loss: 0.7720482446426569\n",
      "Validation Loss: 1.057337902313055\n",
      "Epoch 3, Loss: 0.5784097261900125\n",
      "Validation Loss: 2.4580581742663714\n",
      "Epoch 4, Loss: 0.48958641741164893\n",
      "Validation Loss: 0.9192440662273141\n",
      "Epoch 5, Loss: 0.4308951441631761\n",
      "Validation Loss: 1.3438207465548848\n",
      "Epoch 6, Loss: 0.3974024116299873\n",
      "Validation Loss: 3.6074691927710245\n",
      "Epoch 7, Loss: 0.37367430415957475\n",
      "Validation Loss: 2.946269096330155\n",
      "Epoch 8, Loss: 0.3820636970705764\n",
      "Validation Loss: 0.6693512409232384\n",
      "Epoch 9, Loss: 0.3761375711061234\n",
      "Validation Loss: 6.530225909033487\n",
      "Epoch 10, Loss: 0.40411181470682456\n",
      "Validation Loss: 0.6931641739468242\n",
      "Epoch 11, Loss: 0.36860984858385354\n",
      "Validation Loss: 1.3008412704911343\n",
      "Epoch 12, Loss: 0.354799909127313\n",
      "Validation Loss: 1.268696778042372\n",
      "Epoch 13, Loss: 0.35737161771502607\n",
      "Validation Loss: 1.255784432555354\n",
      "Epoch 14, Loss: 0.36246379412883933\n",
      "Validation Loss: 0.5480415994344756\n",
      "Epoch 15, Loss: 0.34643075875071594\n",
      "Validation Loss: 0.5413320813068124\n",
      "Epoch 16, Loss: 0.32887948131145434\n",
      "Validation Loss: 1.6329813862955846\n",
      "Epoch 17, Loss: 0.32813219223604645\n",
      "Validation Loss: 1.8247110427812088\n",
      "Epoch 18, Loss: 0.31909116270930266\n",
      "Validation Loss: 0.7535318309484527\n",
      "Epoch 19, Loss: 0.3186676744804826\n",
      "Validation Loss: 0.6518057855062707\n",
      "Epoch 20, Loss: 0.31615658173727434\n",
      "Validation Loss: 1.8377877501554267\n",
      "Epoch 21, Loss: 0.3300319821682087\n",
      "Validation Loss: 1.0007372914358628\n",
      "Epoch 22, Loss: 0.3139515716669171\n",
      "Validation Loss: 0.9109073766442233\n",
      "Epoch 23, Loss: 0.30835320783215897\n",
      "Validation Loss: 0.6717210751633311\n",
      "Epoch 24, Loss: 0.3264436448036238\n",
      "Validation Loss: 1.0626600772835488\n",
      "Epoch 25, Loss: 0.3128874116512232\n",
      "Validation Loss: 0.5285030111323955\n",
      "Epoch 26, Loss: 0.296415509699389\n",
      "Validation Loss: 0.5609926137813303\n",
      "Epoch 27, Loss: 0.3104486607534941\n",
      "Validation Loss: 1.1500709389531336\n",
      "Epoch 28, Loss: 0.30799173269160957\n",
      "Validation Loss: 1.7889244750488635\n",
      "Epoch 29, Loss: 0.29634062235438546\n",
      "Validation Loss: 0.8048815436141435\n",
      "Epoch 30, Loss: 0.28906291948501456\n",
      "Validation Loss: 0.339205410244853\n",
      "Epoch 31, Loss: 0.2915899032770201\n",
      "Validation Loss: 1.4069855656734733\n",
      "Epoch 32, Loss: 0.2928001346976258\n",
      "Validation Loss: 0.35652238407800363\n",
      "Epoch 33, Loss: 0.28541110819855403\n",
      "Validation Loss: 0.7186281972153242\n",
      "Epoch 34, Loss: 0.28217979104712954\n",
      "Validation Loss: 0.29106412238852925\n",
      "Epoch 35, Loss: 0.2783605726652367\n",
      "Validation Loss: 1.4589458371317663\n",
      "Epoch 36, Loss: 0.28418416443259215\n",
      "Validation Loss: 0.5122799388197965\n",
      "Epoch 37, Loss: 0.2795802942195604\n",
      "Validation Loss: 0.9924757009328797\n",
      "Epoch 38, Loss: 0.2787571492236714\n",
      "Validation Loss: 1.6745080046875531\n",
      "Epoch 39, Loss: 0.27552495931470117\n",
      "Validation Loss: 0.8991541113964346\n",
      "Epoch 40, Loss: 0.27617754319379495\n",
      "Validation Loss: 0.3233906709870627\n",
      "Epoch 41, Loss: 0.26934546214896576\n",
      "Validation Loss: 0.971215522566507\n",
      "Epoch 42, Loss: 0.27230412097171297\n",
      "Validation Loss: 1.956013607424359\n",
      "Epoch 43, Loss: 0.27069830998431804\n",
      "Validation Loss: 0.8043732934219893\n",
      "Epoch 44, Loss: 0.2661505293014438\n",
      "Validation Loss: 0.318789639791777\n",
      "Epoch 45, Loss: 0.26366141871657484\n",
      "Validation Loss: 0.30167867451213126\n",
      "Epoch 46, Loss: 0.2654993979390277\n",
      "Validation Loss: 2.509580365447111\n",
      "Epoch 47, Loss: 0.2658184430626936\n",
      "Validation Loss: 0.9540783943131913\n",
      "Epoch 48, Loss: 0.26517463371504185\n",
      "Validation Loss: 0.8444775100364241\n",
      "Epoch 49, Loss: 0.26597246025190796\n",
      "Validation Loss: 0.627677185590877\n",
      "Epoch 50, Loss: 0.26947875927354015\n",
      "Validation Loss: 0.26458437948725944\n",
      "Epoch 51, Loss: 0.26690063764189564\n",
      "Validation Loss: 2.487154794293781\n",
      "Epoch 52, Loss: 0.263365518908168\n",
      "Validation Loss: 0.44489646581716313\n",
      "Epoch 53, Loss: 0.2657457428962685\n",
      "Validation Loss: 0.6386229083981625\n",
      "Epoch 54, Loss: 0.2619944687846095\n",
      "Validation Loss: 0.792565104573272\n",
      "Epoch 55, Loss: 0.26844688227703406\n",
      "Validation Loss: 1.3742067120796027\n",
      "Epoch 56, Loss: 0.2645875177411146\n",
      "Validation Loss: 0.6429205668527026\n",
      "Epoch 57, Loss: 0.26909704385108724\n",
      "Validation Loss: 1.3674444542374722\n",
      "Epoch 58, Loss: 0.2762094826199288\n",
      "Validation Loss: 0.4889722979345987\n",
      "Epoch 59, Loss: 0.26866505485634473\n",
      "Validation Loss: 0.3712851003158924\n",
      "Epoch 60, Loss: 0.26763906988293623\n",
      "Validation Loss: 0.7655023516610612\n",
      "Epoch 61, Loss: 0.2659885202382886\n",
      "Validation Loss: 0.8475770215655483\n",
      "Epoch 62, Loss: 0.2720610430074293\n",
      "Validation Loss: 0.5188724336236022\n",
      "Epoch 63, Loss: 0.27084345058646314\n",
      "Validation Loss: 0.5744559647038926\n",
      "Epoch 64, Loss: 0.2652580270240473\n",
      "Validation Loss: 0.6443018788515136\n",
      "Epoch 65, Loss: 0.26741420131090077\n",
      "Validation Loss: 0.8161488681338555\n",
      "Epoch 66, Loss: 0.26642537446216097\n",
      "Validation Loss: 0.9389971730320953\n",
      "Epoch 67, Loss: 0.2645263554051865\n",
      "Validation Loss: 0.6249149434788283\n",
      "Epoch 68, Loss: 0.26132268787816515\n",
      "Validation Loss: 0.5871031194232231\n",
      "Epoch 69, Loss: 0.2635367644387622\n",
      "Validation Loss: 0.45691643482030825\n",
      "Epoch 70, Loss: 0.26264654325191367\n",
      "Validation Loss: 0.549383643754693\n",
      "Epoch 71, Loss: 0.2622038205695707\n",
      "Validation Loss: 1.1158654814542726\n",
      "Epoch 72, Loss: 0.2618141799818638\n",
      "Validation Loss: 0.6041449329187704\n",
      "Epoch 73, Loss: 0.25794461421495263\n",
      "Validation Loss: 0.36965684558069983\n",
      "Epoch 74, Loss: 0.257289809023225\n",
      "Validation Loss: 0.9688815592333327\n",
      "Epoch 75, Loss: 0.25668650345746863\n",
      "Validation Loss: 1.4046350856160008\n",
      "Epoch 76, Loss: 0.2561500924964284\n",
      "Validation Loss: 0.43482417491979375\n",
      "Epoch 77, Loss: 0.25504820481982343\n",
      "Validation Loss: 0.9930456738139308\n",
      "Epoch 78, Loss: 0.25389987039704653\n",
      "Validation Loss: 0.9294758716294932\n",
      "Epoch 79, Loss: 0.2554166450403457\n",
      "Validation Loss: 1.1922853602919468\n",
      "Epoch 80, Loss: 0.25362732036169183\n",
      "Validation Loss: 0.7534013861833617\n",
      "Epoch 81, Loss: 0.25663294227317324\n",
      "Validation Loss: 2.6258793875228528\n",
      "Epoch 82, Loss: 0.25352993919405825\n",
      "Validation Loss: 0.3852739351433377\n",
      "Epoch 83, Loss: 0.25419165991073434\n",
      "Validation Loss: 0.9178794483805812\n",
      "Epoch 84, Loss: 0.2536785842547583\n",
      "Validation Loss: 1.5963946664056112\n",
      "Epoch 85, Loss: 0.2555887197512527\n",
      "Validation Loss: 1.3947724616804789\n",
      "Epoch 86, Loss: 0.2578884831001592\n",
      "Validation Loss: 0.49795536107795185\n",
      "Epoch 87, Loss: 0.25660892887864\n",
      "Validation Loss: 0.36732607802679373\n",
      "Epoch 88, Loss: 0.26044491869072584\n",
      "Validation Loss: 0.6970380690208701\n",
      "Epoch 89, Loss: 0.2559754929916803\n",
      "Validation Loss: 0.3511160556660142\n",
      "Epoch 90, Loss: 0.2573396291497142\n",
      "Validation Loss: 2.2387706734413326\n",
      "Epoch 91, Loss: 0.257426543291225\n",
      "Validation Loss: 1.1118760372317114\n",
      "Epoch 92, Loss: 0.25739860846552737\n",
      "Validation Loss: 0.615195749804031\n",
      "Epoch 93, Loss: 0.25490128006352936\n",
      "Validation Loss: 0.8351186403008395\n",
      "Epoch 94, Loss: 0.2584654267790706\n",
      "Validation Loss: 3.9135045561679576\n",
      "Epoch 95, Loss: 0.26112469493649726\n",
      "Validation Loss: 2.159116869749025\n",
      "Epoch 96, Loss: 0.25798136498345886\n",
      "Validation Loss: 0.7332110224768172\n",
      "Epoch 97, Loss: 0.25348972823730737\n",
      "Validation Loss: 1.4421189413514248\n",
      "Epoch 98, Loss: 0.2526250407099724\n",
      "Validation Loss: 0.41738027957982793\n",
      "Epoch 99, Loss: 0.254017410410005\n",
      "Validation Loss: 0.8629212427970975\n",
      "Epoch 100, Loss: 0.25307927713837736\n",
      "Validation Loss: 0.49218423907146897\n",
      "Epoch 101, Loss: 0.25166832413091217\n",
      "Validation Loss: 1.8613936734753986\n",
      "Epoch 102, Loss: 0.25065185736085094\n",
      "Validation Loss: 1.474373551302178\n",
      "Epoch 103, Loss: 0.2511865214206452\n",
      "Validation Loss: 3.291458634443061\n",
      "Epoch 104, Loss: 0.25107393535070643\n",
      "Validation Loss: 1.008882536444553\n",
      "Epoch 105, Loss: 0.252743718409261\n",
      "Validation Loss: 0.7599184339822724\n",
      "Epoch 106, Loss: 0.25229084890249165\n",
      "Validation Loss: 0.7050650286120038\n",
      "Epoch 107, Loss: 0.2522044327369956\n",
      "Validation Loss: 1.9610551540241685\n",
      "Epoch 108, Loss: 0.2492061933112699\n",
      "Validation Loss: 1.0103222644606302\n",
      "Epoch 109, Loss: 0.2480777269012706\n",
      "Validation Loss: 0.8342237174510956\n",
      "Epoch 110, Loss: 0.24919658795345662\n",
      "Validation Loss: 1.190516846124516\n",
      "Epoch 111, Loss: 0.24599456795772842\n",
      "Validation Loss: 0.6646753528783488\n",
      "Epoch 112, Loss: 0.24862740630673807\n",
      "Validation Loss: 1.8745221315428269\n",
      "Epoch 113, Loss: 0.2475933028861534\n",
      "Validation Loss: 0.5657606277354928\n",
      "Epoch 114, Loss: 0.2479666864802671\n",
      "Validation Loss: 0.5849503888640293\n",
      "Epoch 115, Loss: 0.248355773905682\n",
      "Validation Loss: 1.4964920310086982\n",
      "Epoch 116, Loss: 0.24799214216858842\n",
      "Validation Loss: 2.5831689668256184\n",
      "Epoch 117, Loss: 0.24584388836871746\n",
      "Validation Loss: 4.040665931479875\n",
      "Epoch 118, Loss: 0.24997339501630428\n",
      "Validation Loss: 0.9893973462803419\n",
      "Epoch 119, Loss: 0.24936232594556587\n",
      "Validation Loss: 1.0243177552555882\n",
      "Epoch 120, Loss: 0.24726569652557373\n",
      "Validation Loss: 0.858125815557879\n",
      "Epoch 121, Loss: 0.24649361737592276\n",
      "Validation Loss: 2.584361417348995\n",
      "Epoch 122, Loss: 0.24617092588613199\n",
      "Validation Loss: 1.3603625450023384\n",
      "Epoch 123, Loss: 0.24594541760378105\n",
      "Validation Loss: 2.524685344030691\n",
      "Epoch 124, Loss: 0.24459635856193165\n",
      "Validation Loss: 2.018318495085073\n",
      "Epoch 125, Loss: 0.24584701891208804\n",
      "Validation Loss: 3.0216609710870785\n",
      "Epoch 126, Loss: 0.2504108988268431\n",
      "Validation Loss: 0.4049872281246407\n",
      "Epoch 127, Loss: 0.24624330515778342\n",
      "Validation Loss: 0.5556643917117008\n",
      "Epoch 128, Loss: 0.2498179642093736\n",
      "Validation Loss: 0.8441113081089285\n",
      "Epoch 129, Loss: 0.24728960050053375\n",
      "Validation Loss: 0.569041951104652\n",
      "Epoch 130, Loss: 0.2548417511374451\n",
      "Validation Loss: 0.7964505740376406\n",
      "Epoch 131, Loss: 0.2545458164152711\n",
      "Validation Loss: 2.7601956822151363\n",
      "Epoch 132, Loss: 0.25833602327593536\n",
      "Validation Loss: 0.8093225422293641\n",
      "Epoch 133, Loss: 0.2551144654321116\n",
      "Validation Loss: 0.7396397535191026\n",
      "Epoch 134, Loss: 0.2544467144234236\n",
      "Validation Loss: 1.2460798080577407\n",
      "Epoch 135, Loss: 0.25034705618786257\n",
      "Validation Loss: 1.194980405097784\n",
      "Epoch 136, Loss: 0.24886415924790295\n",
      "Validation Loss: 1.976132728332697\n",
      "Epoch 137, Loss: 0.24831192039473113\n",
      "Validation Loss: 0.44856887328070266\n",
      "Epoch 138, Loss: 0.254433564408574\n",
      "Validation Loss: 0.9478185065957003\n",
      "Epoch 139, Loss: 0.24695239224752716\n",
      "Validation Loss: 1.7656509460404861\n",
      "Epoch 140, Loss: 0.2460364632655022\n",
      "Validation Loss: 0.358498097852219\n",
      "Epoch 141, Loss: 0.24533873935078465\n",
      "Validation Loss: 0.8247244510539743\n",
      "Epoch 142, Loss: 0.24802917891810106\n",
      "Validation Loss: 0.98247101833654\n",
      "Epoch 143, Loss: 0.2443663364406242\n",
      "Validation Loss: 0.29354040931130565\n",
      "Epoch 144, Loss: 0.24209907339062803\n",
      "Validation Loss: 1.5365025456561598\n",
      "Epoch 145, Loss: 0.2420030448499114\n",
      "Validation Loss: 0.9265964869842973\n",
      "Epoch 146, Loss: 0.24209256409559138\n",
      "Validation Loss: 0.3869414229032605\n",
      "Epoch 147, Loss: 0.24499249657572703\n",
      "Validation Loss: 0.5163621548996415\n",
      "Epoch 148, Loss: 0.2781772398671439\n",
      "Validation Loss: 0.8852443196052728\n",
      "Epoch 149, Loss: 0.2657932521298874\n",
      "Validation Loss: 0.8368307878804762\n",
      "Epoch 150, Loss: 0.25603793613439385\n",
      "Validation Loss: 1.4239624752554783\n",
      "Epoch 151, Loss: 0.2559351154364819\n",
      "Validation Loss: 0.47844787600428557\n",
      "Epoch 152, Loss: 0.25497019698106965\n",
      "Validation Loss: 0.44959174616392267\n",
      "Epoch 153, Loss: 0.2500927831197894\n",
      "Validation Loss: 1.090605475181757\n",
      "Epoch 154, Loss: 0.2573562036593293\n",
      "Validation Loss: 1.222030903017798\n",
      "Epoch 155, Loss: 0.2533810714822869\n",
      "Validation Loss: 3.3448233770769695\n",
      "Epoch 156, Loss: 0.2447202472492706\n",
      "Validation Loss: 1.138515340727429\n",
      "Epoch 157, Loss: 0.24689778572944707\n",
      "Validation Loss: 1.6813438354536545\n",
      "Epoch 158, Loss: 0.24439537447205809\n",
      "Validation Loss: 1.244186520576477\n",
      "Epoch 159, Loss: 0.24310884078921274\n",
      "Validation Loss: 1.4663627729859463\n",
      "Epoch 160, Loss: 0.24211027005384134\n",
      "Validation Loss: 1.6716716289520264\n",
      "Epoch 161, Loss: 0.24139828832690105\n",
      "Validation Loss: 1.4238229629605315\n",
      "Epoch 162, Loss: 0.24009730597568113\n",
      "Validation Loss: 1.4483811633531438\n",
      "Epoch 163, Loss: 0.2420174003860285\n",
      "Validation Loss: 1.9362767851629923\n",
      "Epoch 164, Loss: 0.24243221639893775\n",
      "Validation Loss: 0.7397131018860396\n",
      "Epoch 165, Loss: 0.24042592266964358\n",
      "Validation Loss: 1.7647870418637297\n",
      "Epoch 166, Loss: 0.2403955955144971\n",
      "Validation Loss: 1.0914185047149658\n",
      "Epoch 167, Loss: 0.243050254621478\n",
      "Validation Loss: 2.0505113352176756\n",
      "Epoch 168, Loss: 0.2543972877915515\n",
      "Validation Loss: 1.55182269423507\n",
      "Epoch 169, Loss: 0.2700717471713244\n",
      "Validation Loss: 3.685609080070673\n",
      "Epoch 170, Loss: 0.2536195700251779\n",
      "Validation Loss: 1.296066913493844\n",
      "Epoch 171, Loss: 0.24276751054580822\n",
      "Validation Loss: 0.43805529419765915\n",
      "Epoch 172, Loss: 0.2432494444209476\n",
      "Validation Loss: 1.1465346730032633\n",
      "Epoch 173, Loss: 0.24088135283700254\n",
      "Validation Loss: 1.756774395011192\n",
      "Epoch 174, Loss: 0.24018974004443303\n",
      "Validation Loss: 1.619246940280116\n",
      "Epoch 175, Loss: 0.24073815588341202\n",
      "Validation Loss: 1.0468323452528132\n",
      "Epoch 176, Loss: 0.23958213817934657\n",
      "Validation Loss: 2.192406313363896\n",
      "Epoch 177, Loss: 0.23997733092238738\n",
      "Validation Loss: 0.8794498083203338\n",
      "Epoch 178, Loss: 0.23949429184891458\n",
      "Validation Loss: 2.353651615076287\n",
      "Epoch 179, Loss: 0.23917659406745156\n",
      "Validation Loss: 0.6947206265704576\n",
      "Epoch 180, Loss: 0.23775531758749208\n",
      "Validation Loss: 1.9397002763526385\n",
      "Epoch 181, Loss: 0.23816356522052787\n",
      "Validation Loss: 0.782869336910026\n",
      "Epoch 182, Loss: 0.23694216399345286\n",
      "Validation Loss: 1.5963392534921335\n",
      "Epoch 183, Loss: 0.23672131674234256\n",
      "Validation Loss: 0.8973377083623132\n",
      "Epoch 184, Loss: 0.23596158675676168\n",
      "Validation Loss: 2.024480936139129\n",
      "Epoch 185, Loss: 0.23603782248358393\n",
      "Validation Loss: 0.9430637969527134\n",
      "Epoch 186, Loss: 0.23600098516705426\n",
      "Validation Loss: 2.5108718400777774\n",
      "Epoch 187, Loss: 0.23564475150995476\n",
      "Validation Loss: 1.401618634545526\n",
      "Epoch 188, Loss: 0.23569376090931338\n",
      "Validation Loss: 1.6477926221004753\n",
      "Epoch 189, Loss: 0.23568990800616352\n",
      "Validation Loss: 1.4002781829168631\n",
      "Epoch 190, Loss: 0.23498572556431904\n",
      "Validation Loss: 1.4873765817908353\n",
      "Epoch 191, Loss: 0.23454967691281506\n",
      "Validation Loss: 1.342902568883674\n",
      "Epoch 192, Loss: 0.23430055137290512\n",
      "Validation Loss: 1.8092962947002678\n",
      "Epoch 193, Loss: 0.23485318692617638\n",
      "Validation Loss: 1.3982776375704034\n",
      "Epoch 194, Loss: 0.234044884872991\n",
      "Validation Loss: 1.3710610852685086\n",
      "Epoch 195, Loss: 0.23403711655978546\n",
      "Validation Loss: 1.7437689359797988\n",
      "Epoch 196, Loss: 0.23368318149343478\n",
      "Validation Loss: 1.2905997620072476\n",
      "Epoch 197, Loss: 0.23339255682604257\n",
      "Validation Loss: 1.7818323068840558\n",
      "Epoch 198, Loss: 0.23334977546230304\n",
      "Validation Loss: 1.5057417875112489\n",
      "Epoch 199, Loss: 0.23279842500423276\n",
      "Validation Loss: 1.6801192954529163\n",
      "Epoch 200, Loss: 0.23259700159000796\n",
      "Validation Loss: 1.7761841180712679\n",
      "Epoch 201, Loss: 0.23397336683647577\n",
      "Validation Loss: 0.5444135319354922\n",
      "Epoch 202, Loss: 0.23298281981328198\n",
      "Validation Loss: 3.527587325073952\n",
      "Epoch 203, Loss: 0.2364640425197607\n",
      "Validation Loss: 1.0625981563745543\n",
      "Epoch 204, Loss: 0.23406000201438748\n",
      "Validation Loss: 2.077883016231448\n",
      "Epoch 205, Loss: 0.2335642007461121\n",
      "Validation Loss: 2.4225442104561385\n",
      "Epoch 206, Loss: 0.23264944107206756\n",
      "Validation Loss: 1.286777869213459\n",
      "Epoch 207, Loss: 0.23217825465943925\n",
      "Validation Loss: 1.3975541397582654\n",
      "Epoch 208, Loss: 0.23207834393305835\n",
      "Validation Loss: 1.326945722103119\n",
      "Epoch 209, Loss: 0.23187555784229621\n",
      "Validation Loss: 2.675836518753407\n",
      "Epoch 210, Loss: 0.23215211343106834\n",
      "Validation Loss: 1.953845786493878\n",
      "Epoch 211, Loss: 0.23185804031442764\n",
      "Validation Loss: 1.5105360774106757\n",
      "Epoch 212, Loss: 0.2315633735771096\n",
      "Validation Loss: 2.050648617189984\n",
      "Epoch 213, Loss: 0.23133089423699435\n",
      "Validation Loss: 1.4960597878278687\n",
      "Epoch 214, Loss: 0.23168446617417557\n",
      "Validation Loss: 1.5556326799614484\n",
      "Epoch 215, Loss: 0.23144204316790715\n",
      "Validation Loss: 1.1045499762823416\n",
      "Epoch 216, Loss: 0.2310956043386182\n",
      "Validation Loss: 2.257604391075844\n",
      "Epoch 217, Loss: 0.23149510464349457\n",
      "Validation Loss: 1.2718080074288125\n",
      "Epoch 218, Loss: 0.23103472477821416\n",
      "Validation Loss: 1.6778791477513868\n",
      "Epoch 219, Loss: 0.2305857090409412\n",
      "Validation Loss: 1.299850459708724\n",
      "Epoch 220, Loss: 0.22987579381050066\n",
      "Validation Loss: 1.4641358270201572\n",
      "Epoch 221, Loss: 0.23040131423189197\n",
      "Validation Loss: 0.6735278069972992\n",
      "Epoch 222, Loss: 0.23150865170498228\n",
      "Validation Loss: 0.7332734264606653\n",
      "Epoch 223, Loss: 0.23585167007390842\n",
      "Validation Loss: 2.692389740500339\n",
      "Epoch 224, Loss: 0.23326477702966955\n",
      "Validation Loss: 0.9018337185992751\n",
      "Epoch 225, Loss: 0.23170509995069616\n",
      "Validation Loss: 0.7740565004736878\n",
      "Epoch 226, Loss: 0.22999278581592925\n",
      "Validation Loss: 1.8616503976112189\n",
      "Epoch 227, Loss: 0.22924275055181148\n",
      "Validation Loss: 0.8332448366076447\n",
      "Epoch 228, Loss: 0.22856800063231655\n",
      "Validation Loss: 1.428303024103475\n",
      "Epoch 229, Loss: 0.2287692241370678\n",
      "Validation Loss: 1.4731407026911891\n",
      "Epoch 230, Loss: 0.2283474851919468\n",
      "Validation Loss: 1.0200794450072355\n",
      "Epoch 231, Loss: 0.22812429295722828\n",
      "Validation Loss: 1.0283221560855245\n",
      "Epoch 232, Loss: 0.22823650870732096\n",
      "Validation Loss: 1.4091512228167333\n",
      "Epoch 233, Loss: 0.22801572374652984\n",
      "Validation Loss: 0.6829641738603281\n",
      "Epoch 234, Loss: 0.2283652682983598\n",
      "Validation Loss: 1.4764933627705241\n",
      "Epoch 235, Loss: 0.2285486229283865\n",
      "Validation Loss: 2.7282964989196423\n",
      "Epoch 236, Loss: 0.22771360859448134\n",
      "Validation Loss: 1.36998945890471\n",
      "Epoch 237, Loss: 0.22806905890100224\n",
      "Validation Loss: 1.8736580111259638\n",
      "Epoch 238, Loss: 0.22722791884700919\n",
      "Validation Loss: 1.1585135681684626\n",
      "Epoch 239, Loss: 0.22713967967172002\n",
      "Validation Loss: 1.2800341373266175\n",
      "Epoch 240, Loss: 0.22716153053523497\n",
      "Validation Loss: 2.109636118245679\n",
      "Epoch 241, Loss: 0.22731865517968355\n",
      "Validation Loss: 1.4838395382082739\n",
      "Epoch 242, Loss: 0.22644946719844675\n",
      "Validation Loss: 2.1215020390444024\n",
      "Epoch 243, Loss: 0.22680090090563132\n",
      "Validation Loss: 1.5346038175183674\n",
      "Epoch 244, Loss: 0.22693097158226855\n",
      "Validation Loss: 1.290482950765033\n",
      "Epoch 245, Loss: 0.22683980848726837\n",
      "Validation Loss: 1.323847658412401\n",
      "Epoch 246, Loss: 0.22623264750595704\n",
      "Validation Loss: 0.9367572207783543\n",
      "Epoch 247, Loss: 0.22599114274042983\n",
      "Validation Loss: 0.9609725918880728\n",
      "Epoch 248, Loss: 0.22536331809364085\n",
      "Validation Loss: 1.4623367453730383\n",
      "Epoch 249, Loss: 0.2256124495680249\n",
      "Validation Loss: 1.344182038029959\n",
      "Epoch 250, Loss: 0.22563727594218974\n",
      "Validation Loss: 1.2730017262835835\n",
      "Epoch 251, Loss: 0.22540712269932725\n",
      "Validation Loss: 1.1727352225503256\n",
      "Epoch 252, Loss: 0.22498407920952454\n",
      "Validation Loss: 1.0667960990306944\n",
      "Epoch 253, Loss: 0.22532916285617408\n",
      "Validation Loss: 1.7005749552748923\n",
      "Epoch 254, Loss: 0.22480536429861256\n",
      "Validation Loss: 0.8609694679116093\n",
      "Epoch 255, Loss: 0.22434844899662706\n",
      "Validation Loss: 1.6181667793628782\n",
      "Epoch 256, Loss: 0.2251811545602111\n",
      "Validation Loss: 1.3071614739506743\n",
      "Epoch 257, Loss: 0.22471078277327294\n",
      "Validation Loss: 1.0787185596865276\n",
      "Epoch 258, Loss: 0.22445598296647848\n",
      "Validation Loss: 1.0208614648774612\n",
      "Epoch 259, Loss: 0.2248247528751922\n",
      "Validation Loss: 1.3951930362124776\n",
      "Epoch 260, Loss: 0.2242780676501435\n",
      "Validation Loss: 2.0039235769316206\n",
      "Epoch 261, Loss: 0.22398885932945928\n",
      "Validation Loss: 1.3504891908446024\n",
      "Epoch 262, Loss: 0.22408752173705157\n",
      "Validation Loss: 1.8465340331543323\n",
      "Epoch 263, Loss: 0.22384644966832426\n",
      "Validation Loss: 1.8160996658857478\n",
      "Epoch 264, Loss: 0.2234998448037131\n",
      "Validation Loss: 1.7980205846387287\n",
      "Epoch 265, Loss: 0.22320410431644253\n",
      "Validation Loss: 1.4788343199463778\n",
      "Epoch 266, Loss: 0.22323321382146935\n",
      "Validation Loss: 1.1310852987821711\n",
      "Epoch 267, Loss: 0.2224116775445467\n",
      "Validation Loss: 1.281693084295406\n",
      "Epoch 268, Loss: 0.2222222541567198\n",
      "Validation Loss: 2.0116888589637223\n",
      "Epoch 269, Loss: 0.2235432793494574\n",
      "Validation Loss: 1.8228952524273894\n",
      "Epoch 270, Loss: 0.2243039772174386\n",
      "Validation Loss: 1.5094629290492036\n",
      "Epoch 271, Loss: 0.2245768919413866\n",
      "Validation Loss: 2.1864900284035262\n",
      "Epoch 272, Loss: 0.22908292035030764\n",
      "Validation Loss: 0.5155731786129086\n",
      "Epoch 273, Loss: 0.22612875617694023\n",
      "Validation Loss: 2.59630277822184\n",
      "Epoch 274, Loss: 0.22394171192548995\n",
      "Validation Loss: 1.187578417534052\n",
      "Epoch 275, Loss: 0.2227631177493306\n",
      "Validation Loss: 0.989452346812847\n",
      "Epoch 276, Loss: 0.22247031824879868\n",
      "Validation Loss: 1.6592426216879557\n",
      "Epoch 277, Loss: 0.2228191636156204\n",
      "Validation Loss: 3.4350431020869765\n",
      "Epoch 278, Loss: 0.22408996431460215\n",
      "Validation Loss: 1.5812798946402793\n",
      "Epoch 279, Loss: 0.22241087045607177\n",
      "Validation Loss: 1.5982051727383635\n",
      "Epoch 280, Loss: 0.22181635839474756\n",
      "Validation Loss: 1.1262220366056575\n",
      "Epoch 281, Loss: 0.22223641160269117\n",
      "Validation Loss: 1.1604691688404527\n",
      "Epoch 282, Loss: 0.22153352304946544\n",
      "Validation Loss: 1.519183494323908\n",
      "Epoch 283, Loss: 0.22330185251180515\n",
      "Validation Loss: 0.8771749235862909\n",
      "Epoch 284, Loss: 0.22133816193875877\n",
      "Validation Loss: 0.7956992644210195\n",
      "Epoch 285, Loss: 0.22102120362742003\n",
      "Validation Loss: 1.1832644897837972\n",
      "Epoch 286, Loss: 0.2210989433752243\n",
      "Validation Loss: 1.271809767845065\n",
      "Epoch 287, Loss: 0.22074394543157067\n",
      "Validation Loss: 0.6160014576690142\n",
      "Epoch 288, Loss: 0.22000095941299616\n",
      "Validation Loss: 0.6460201095703036\n",
      "Epoch 289, Loss: 0.22125391848385334\n",
      "Validation Loss: 0.6700200376122497\n",
      "Epoch 290, Loss: 0.22020626115764297\n",
      "Validation Loss: 0.7156491771686909\n",
      "Epoch 291, Loss: 0.2199137879448921\n",
      "Validation Loss: 0.4765119597662327\n",
      "Epoch 292, Loss: 0.2192996134331753\n",
      "Validation Loss: 0.7720070599123489\n",
      "Epoch 293, Loss: 0.21970981402799142\n",
      "Validation Loss: 1.028956536636796\n",
      "Epoch 294, Loss: 0.21949342967465865\n",
      "Validation Loss: 1.0919186278831128\n",
      "Epoch 295, Loss: 0.21878834261537292\n",
      "Validation Loss: 0.8286549254905345\n",
      "Epoch 296, Loss: 0.2187087718657283\n",
      "Validation Loss: 0.9956919814265052\n",
      "Epoch 297, Loss: 0.21967741574139096\n",
      "Validation Loss: 0.8822206688481707\n",
      "Epoch 298, Loss: 0.21817283574924912\n",
      "Validation Loss: 0.6850086616915326\n",
      "Epoch 299, Loss: 0.21791736625654753\n",
      "Validation Loss: 1.0174258309741353\n",
      "Epoch 300, Loss: 0.2196990402718616\n",
      "Validation Loss: 1.1750102556029032\n",
      "Epoch 301, Loss: 0.21826559259707845\n",
      "Validation Loss: 0.5506399980811185\n",
      "Epoch 302, Loss: 0.21820891120059546\n",
      "Validation Loss: 1.316892654396767\n",
      "Epoch 303, Loss: 0.21745198133379914\n",
      "Validation Loss: 1.2057959839355115\n",
      "Epoch 304, Loss: 0.2182643245689051\n",
      "Validation Loss: 1.2662917472595392\n",
      "Epoch 305, Loss: 0.2176196371789935\n",
      "Validation Loss: 0.6484001486800438\n",
      "Epoch 306, Loss: 0.21725760883283476\n",
      "Validation Loss: 0.8562320429225301\n",
      "Epoch 307, Loss: 0.21747169955525286\n",
      "Validation Loss: 0.7133371996325116\n",
      "Epoch 308, Loss: 0.21754782690211785\n",
      "Validation Loss: 0.9026625807895217\n",
      "Epoch 309, Loss: 0.21760516713351705\n",
      "Validation Loss: 0.9988394837046779\n",
      "Epoch 310, Loss: 0.21715959772294344\n",
      "Validation Loss: 0.8694503224173258\n",
      "Epoch 311, Loss: 0.21724263907865035\n",
      "Validation Loss: 1.148066947626513\n",
      "Epoch 312, Loss: 0.217035683824919\n",
      "Validation Loss: 0.5973970106867856\n",
      "Epoch 313, Loss: 0.21771526202386202\n",
      "Validation Loss: 0.6308346238247183\n",
      "Epoch 314, Loss: 0.21717506755403307\n",
      "Validation Loss: 0.928349960682004\n",
      "Epoch 315, Loss: 0.2167611615299139\n",
      "Validation Loss: 0.784679633240367\n",
      "Epoch 316, Loss: 0.2165054041069261\n",
      "Validation Loss: 0.9039063897243765\n",
      "Epoch 317, Loss: 0.2170648108734641\n",
      "Validation Loss: 0.7952741516190905\n",
      "Epoch 318, Loss: 0.2160664237385919\n",
      "Validation Loss: 0.7655193992825442\n",
      "Epoch 319, Loss: 0.2150798312626606\n",
      "Validation Loss: 1.0844027552493782\n",
      "Epoch 320, Loss: 0.21597524940274482\n",
      "Validation Loss: 2.3780290165612863\n",
      "Epoch 321, Loss: 0.21662815898483576\n",
      "Validation Loss: 1.752305410629095\n",
      "Epoch 322, Loss: 0.21729702688753605\n",
      "Validation Loss: 1.2834189811418222\n",
      "Epoch 323, Loss: 0.2217333463648724\n",
      "Validation Loss: 0.6002809897411702\n",
      "Epoch 324, Loss: 0.21619286717370498\n",
      "Validation Loss: 1.788313488627589\n",
      "Epoch 325, Loss: 0.21554648616286212\n",
      "Validation Loss: 1.9446220647457033\n",
      "Epoch 326, Loss: 0.2162974224057655\n",
      "Validation Loss: 1.1345235996468122\n",
      "Epoch 327, Loss: 0.21607483711180298\n",
      "Validation Loss: 1.3480779791987219\n",
      "Epoch 328, Loss: 0.2155677038569783\n",
      "Validation Loss: 1.1288310206213663\n",
      "Epoch 329, Loss: 0.21504452590679013\n",
      "Validation Loss: 1.338260560534721\n",
      "Epoch 330, Loss: 0.21531010928108943\n",
      "Validation Loss: 0.9458289243454157\n",
      "Epoch 331, Loss: 0.21606766475841058\n",
      "Validation Loss: 0.7578990972319315\n",
      "Epoch 332, Loss: 0.21451812724821095\n",
      "Validation Loss: 1.0188569232474927\n",
      "Epoch 333, Loss: 0.21482669297865656\n",
      "Validation Loss: 0.8969359980073086\n",
      "Epoch 334, Loss: 0.2155337343642185\n",
      "Validation Loss: 1.0365685751271803\n",
      "Epoch 335, Loss: 0.21491130268157915\n",
      "Validation Loss: 0.7512099174566047\n",
      "Epoch 336, Loss: 0.21506863459944725\n",
      "Validation Loss: 0.7495288051838098\n",
      "Epoch 337, Loss: 0.2149889504233765\n",
      "Validation Loss: 0.5968944374905076\n",
      "Epoch 338, Loss: 0.21510196927674982\n",
      "Validation Loss: 0.369693661498469\n",
      "Epoch 339, Loss: 0.21499618844583976\n",
      "Validation Loss: 0.7766744951869167\n",
      "Epoch 340, Loss: 0.213597136138137\n",
      "Validation Loss: 0.5655669375907543\n",
      "Epoch 341, Loss: 0.2140637721953004\n",
      "Validation Loss: 0.4172258394402127\n",
      "Epoch 342, Loss: 0.21400738919023857\n",
      "Validation Loss: 0.6186866489953773\n",
      "Epoch 343, Loss: 0.2144551485733584\n",
      "Validation Loss: 0.8936141847177993\n",
      "Epoch 344, Loss: 0.2166192314349288\n",
      "Validation Loss: 0.6877816866996677\n",
      "Epoch 345, Loss: 0.2172680385843959\n",
      "Validation Loss: 0.49603493684946104\n",
      "Epoch 346, Loss: 0.21526130711183297\n",
      "Validation Loss: 0.657867428868316\n",
      "Epoch 347, Loss: 0.21365624373822018\n",
      "Validation Loss: 1.6011867107347\n",
      "Epoch 348, Loss: 0.21337445500458396\n",
      "Validation Loss: 1.1069606431694918\n",
      "Epoch 349, Loss: 0.2145667890476626\n",
      "Validation Loss: 0.505858069242433\n",
      "Epoch 350, Loss: 0.21425394107436024\n",
      "Validation Loss: 0.7648294276969377\n",
      "Epoch 351, Loss: 0.2139152683274344\n",
      "Validation Loss: 0.2786693186607472\n",
      "Epoch 352, Loss: 0.21303773643232363\n",
      "Validation Loss: 0.5138883597629015\n",
      "Epoch 353, Loss: 0.21285561959497457\n",
      "Validation Loss: 0.5524029371350311\n",
      "Epoch 354, Loss: 0.21217741672122895\n",
      "Validation Loss: 0.6081399938394857\n",
      "Epoch 355, Loss: 0.21273884133890616\n",
      "Validation Loss: 0.440300488194754\n",
      "Epoch 356, Loss: 0.21254611684572558\n",
      "Validation Loss: 0.504686449156251\n",
      "Epoch 357, Loss: 0.21417154534178418\n",
      "Validation Loss: 0.5628213889377062\n",
      "Epoch 358, Loss: 0.2133656877287945\n",
      "Validation Loss: 0.42203984946705575\n",
      "Epoch 359, Loss: 0.21396859837046198\n",
      "Validation Loss: 0.5361362272916839\n",
      "Epoch 360, Loss: 0.2125194728980924\n",
      "Validation Loss: 0.4758595831172411\n",
      "Epoch 361, Loss: 0.2121877581704148\n",
      "Validation Loss: 0.5913281496181044\n",
      "Epoch 362, Loss: 0.21171819635255393\n",
      "Validation Loss: 1.1084223528241002\n",
      "Epoch 363, Loss: 0.21345968163290688\n",
      "Validation Loss: 0.44961180867150774\n",
      "Epoch 364, Loss: 0.2121411659429933\n",
      "Validation Loss: 0.44771518229052076\n",
      "Epoch 365, Loss: 0.2129194164258796\n",
      "Validation Loss: 0.4265039355255837\n",
      "Epoch 366, Loss: 0.21230909065884906\n",
      "Validation Loss: 0.3686500888231189\n",
      "Epoch 367, Loss: 0.2128706752907398\n",
      "Validation Loss: 0.4691784797712814\n",
      "Epoch 368, Loss: 0.2122434486396784\n",
      "Validation Loss: 0.7512334536674411\n",
      "Epoch 369, Loss: 0.21223210051742403\n",
      "Validation Loss: 0.4686151843431384\n",
      "Epoch 370, Loss: 0.21172701955101517\n",
      "Validation Loss: 0.5041406674440517\n",
      "Epoch 371, Loss: 0.21191936098905498\n",
      "Validation Loss: 0.6345462022825729\n",
      "Epoch 372, Loss: 0.21144754456919293\n",
      "Validation Loss: 0.3516345519659131\n",
      "Epoch 373, Loss: 0.21203672651981198\n",
      "Validation Loss: 0.39688977321913077\n",
      "Epoch 374, Loss: 0.22383765811317188\n",
      "Validation Loss: 1.6292132513467656\n",
      "Epoch 375, Loss: 0.21901192529083685\n",
      "Validation Loss: 6.562339472216229\n",
      "Epoch 376, Loss: 0.21550328310492428\n",
      "Validation Loss: 3.5664487273194068\n",
      "Epoch 377, Loss: 0.21425233036279678\n",
      "Validation Loss: 1.4254129057706788\n",
      "Epoch 378, Loss: 0.2117826282371615\n",
      "Validation Loss: 0.37263450442358503\n",
      "Epoch 379, Loss: 0.21037606370830259\n",
      "Validation Loss: 0.42736611844495287\n",
      "Epoch 380, Loss: 0.2104214322211784\n",
      "Validation Loss: 0.4625557609075724\n",
      "Epoch 381, Loss: 0.2102784447935085\n",
      "Validation Loss: 0.6347663513449735\n",
      "Epoch 382, Loss: 0.2092836359732373\n",
      "Validation Loss: 0.322877274696217\n",
      "Epoch 383, Loss: 0.20935322476420984\n",
      "Validation Loss: 0.4571409797252611\n",
      "Epoch 384, Loss: 0.21013192977582992\n",
      "Validation Loss: 0.3897621704395427\n",
      "Epoch 385, Loss: 0.2105637040249137\n",
      "Validation Loss: 0.7542210969814035\n",
      "Epoch 386, Loss: 0.2096172285504466\n",
      "Validation Loss: 0.8580057579417562\n",
      "Epoch 387, Loss: 0.2100820105392919\n",
      "Validation Loss: 1.1666246125864428\n",
      "Epoch 388, Loss: 0.20862445482161157\n",
      "Validation Loss: 0.5144577868456064\n",
      "Epoch 389, Loss: 0.2093414045047275\n",
      "Validation Loss: 0.5041581704172977\n",
      "Epoch 390, Loss: 0.20960604655014914\n",
      "Validation Loss: 1.1273756068806315\n",
      "Epoch 391, Loss: 0.2104812012768762\n",
      "Validation Loss: 0.4001803789720979\n",
      "Epoch 392, Loss: 0.20879377606649732\n",
      "Validation Loss: 1.1239876982777617\n",
      "Epoch 393, Loss: 0.20806374452834905\n",
      "Validation Loss: 0.5370207158632057\n",
      "Epoch 394, Loss: 0.2084417628600847\n",
      "Validation Loss: 0.45751045990821926\n",
      "Epoch 395, Loss: 0.20719805195234542\n",
      "Validation Loss: 0.45727662568868593\n",
      "Epoch 396, Loss: 0.2091369027445136\n",
      "Validation Loss: 0.4962292653183604\n",
      "Epoch 397, Loss: 0.2076768529666371\n",
      "Validation Loss: 0.4055050223372703\n",
      "Epoch 398, Loss: 0.20903082538482753\n",
      "Validation Loss: 0.5725434325462164\n",
      "Epoch 399, Loss: 0.20768178046442742\n",
      "Validation Loss: 0.45651547957298366\n",
      "Epoch 400, Loss: 0.21360077979692885\n",
      "Validation Loss: 0.795974787584571\n",
      "Epoch 401, Loss: 0.2086231886994007\n",
      "Validation Loss: 0.4013144363497579\n",
      "Epoch 402, Loss: 0.20867136597286823\n",
      "Validation Loss: 0.5771557044151218\n",
      "Epoch 403, Loss: 0.20775817618380452\n",
      "Validation Loss: 0.4323626146760098\n",
      "Epoch 404, Loss: 0.20698512583797754\n",
      "Validation Loss: 0.430687103160592\n",
      "Epoch 405, Loss: 0.2066479254254075\n",
      "Validation Loss: 0.4198511818120646\n",
      "Epoch 406, Loss: 0.20799180154883584\n",
      "Validation Loss: 0.46645688178927397\n",
      "Epoch 407, Loss: 0.2076933374411838\n",
      "Validation Loss: 0.7140006249727204\n",
      "Epoch 408, Loss: 0.20658325646505799\n",
      "Validation Loss: 0.3359196903400643\n",
      "Epoch 409, Loss: 0.2065668364033796\n",
      "Validation Loss: 0.33082420049711714\n",
      "Epoch 410, Loss: 0.20700139959537706\n",
      "Validation Loss: 0.4447794386813807\n",
      "Epoch 411, Loss: 0.2091351407038611\n",
      "Validation Loss: 0.3892088224028432\n",
      "Epoch 412, Loss: 0.20632730307447356\n",
      "Validation Loss: 0.4308987592541894\n",
      "Epoch 413, Loss: 0.20626450403658456\n",
      "Validation Loss: 0.4046011206715606\n",
      "Epoch 414, Loss: 0.20746512795516917\n",
      "Validation Loss: 0.4368027334296426\n",
      "Epoch 415, Loss: 0.20705185022724923\n",
      "Validation Loss: 0.35801973938941956\n",
      "Epoch 416, Loss: 0.206551706938203\n",
      "Validation Loss: 0.4028025626443153\n",
      "Epoch 417, Loss: 0.20586326144375774\n",
      "Validation Loss: 0.3768077842717947\n",
      "Epoch 418, Loss: 0.20601046256461117\n",
      "Validation Loss: 0.4520588274611983\n",
      "Epoch 419, Loss: 0.20615550706726174\n",
      "Validation Loss: 0.39707214714482775\n",
      "Epoch 420, Loss: 0.20871579638400742\n",
      "Validation Loss: 1.5885273010231729\n",
      "Epoch 421, Loss: 0.2735956494891366\n",
      "Validation Loss: 3.5006324690441755\n",
      "Epoch 422, Loss: 0.23527158983051777\n",
      "Validation Loss: 1.3698406954144322\n",
      "Epoch 423, Loss: 0.2223249660848185\n",
      "Validation Loss: 0.5043159848035768\n",
      "Epoch 424, Loss: 0.21606953359793785\n",
      "Validation Loss: 0.35949612876703574\n",
      "Epoch 425, Loss: 0.21786495830950348\n",
      "Validation Loss: 0.7777456357035526\n",
      "Epoch 426, Loss: 0.21333966070656166\n",
      "Validation Loss: 1.5017395768054695\n",
      "Epoch 427, Loss: 0.21171385270738324\n",
      "Validation Loss: 1.6938062351803447\n",
      "Epoch 428, Loss: 0.21084145941706592\n",
      "Validation Loss: 0.528850313882495\n",
      "Epoch 429, Loss: 0.20959441467773082\n",
      "Validation Loss: 0.602528395347817\n",
      "Epoch 430, Loss: 0.20914941610291946\n",
      "Validation Loss: 1.0293239729349004\n",
      "Epoch 431, Loss: 0.20874164197160755\n",
      "Validation Loss: 0.6173953649609588\n",
      "Epoch 432, Loss: 0.20832618226318858\n",
      "Validation Loss: 0.3128418399150981\n",
      "Epoch 433, Loss: 0.20808046926246132\n",
      "Validation Loss: 0.6270415422528289\n",
      "Epoch 434, Loss: 0.20727540097784164\n",
      "Validation Loss: 0.29705987281577534\n",
      "Epoch 435, Loss: 0.20780294111301734\n",
      "Validation Loss: 0.4966729352640551\n",
      "Epoch 436, Loss: 0.20678458782995857\n",
      "Validation Loss: 0.4311231253452079\n",
      "Epoch 437, Loss: 0.20701809201476185\n",
      "Validation Loss: 0.6534000260885372\n",
      "Epoch 438, Loss: 0.2069871092951575\n",
      "Validation Loss: 0.7188781756301259\n",
      "Epoch 439, Loss: 0.20652350540771042\n",
      "Validation Loss: 0.53551735503729\n",
      "Epoch 440, Loss: 0.20686867529916209\n",
      "Validation Loss: 0.5407342775616535\n",
      "Epoch 441, Loss: 0.2073078131259874\n",
      "Validation Loss: 0.4682872035475664\n",
      "Epoch 442, Loss: 0.20647933048217795\n",
      "Validation Loss: 0.37446405790572945\n",
      "Epoch 443, Loss: 0.20677602884554586\n",
      "Validation Loss: 0.3940848801718202\n",
      "Epoch 444, Loss: 0.20608211481987043\n",
      "Validation Loss: 0.4974951660910318\n",
      "Epoch 445, Loss: 0.20557764527756114\n",
      "Validation Loss: 0.2800591943222423\n",
      "Epoch 446, Loss: 0.20613766998745675\n",
      "Validation Loss: 0.3861009031534195\n",
      "Epoch 447, Loss: 0.205982766005882\n",
      "Validation Loss: 0.3716133410154387\n",
      "Epoch 448, Loss: 0.20607448824096558\n",
      "Validation Loss: 0.5187454625617626\n",
      "Epoch 449, Loss: 0.20557937194961448\n",
      "Validation Loss: 0.3207841311083284\n",
      "Epoch 450, Loss: 0.20535397339005804\n",
      "Validation Loss: 0.3437242747046227\n",
      "Epoch 451, Loss: 0.20588390970992487\n",
      "Validation Loss: 0.3931911909996077\n",
      "Epoch 452, Loss: 0.2052059272780668\n",
      "Validation Loss: 0.3741405141908069\n",
      "Epoch 453, Loss: 0.20563855342740237\n",
      "Validation Loss: 0.32894551788651666\n",
      "Epoch 454, Loss: 0.205203304587062\n",
      "Validation Loss: 0.36908101411752925\n",
      "Epoch 455, Loss: 0.20470342668163222\n",
      "Validation Loss: 0.3495011239550834\n",
      "Epoch 456, Loss: 0.20509304192870162\n",
      "Validation Loss: 0.3903673665468083\n",
      "Epoch 457, Loss: 0.20521535070309804\n",
      "Validation Loss: 0.34495213177315026\n",
      "Epoch 458, Loss: 0.20563382906622665\n",
      "Validation Loss: 0.3324372636717419\n",
      "Epoch 459, Loss: 0.20478433348931546\n",
      "Validation Loss: 0.46732958943344827\n",
      "Epoch 460, Loss: 0.20505550763634747\n",
      "Validation Loss: 0.3798856686714084\n",
      "Epoch 461, Loss: 0.20433198153799356\n",
      "Validation Loss: 0.4492012075213499\n",
      "Epoch 462, Loss: 0.20437299239254275\n",
      "Validation Loss: 0.42415505127851355\n",
      "Epoch 463, Loss: 0.20429386103222535\n",
      "Validation Loss: 0.38349234884561495\n",
      "Epoch 464, Loss: 0.2052368282145539\n",
      "Validation Loss: 0.4570251020581223\n",
      "Epoch 465, Loss: 0.20432357429418452\n",
      "Validation Loss: 0.4134072443080503\n",
      "Epoch 466, Loss: 0.2039520429923784\n",
      "Validation Loss: 0.41968599862830586\n",
      "Epoch 467, Loss: 0.20374225909540125\n",
      "Validation Loss: 0.41128882592500643\n",
      "Epoch 468, Loss: 0.20383183978671252\n",
      "Validation Loss: 0.5165491062541341\n",
      "Epoch 469, Loss: 0.2047516942284135\n",
      "Validation Loss: 0.45372380385565203\n",
      "Epoch 470, Loss: 0.20501558616930662\n",
      "Validation Loss: 0.6563838921314062\n",
      "Epoch 471, Loss: 0.2049093886256911\n",
      "Validation Loss: 0.5723197654236195\n",
      "Epoch 472, Loss: 0.20421909536559915\n",
      "Validation Loss: 0.473512061113535\n",
      "Epoch 473, Loss: 0.20485060913271683\n",
      "Validation Loss: 0.39263065644474915\n",
      "Epoch 474, Loss: 0.20407594272563623\n",
      "Validation Loss: 0.3431238133546918\n",
      "Epoch 475, Loss: 0.20499807085062183\n",
      "Validation Loss: 0.5016669880512149\n",
      "Epoch 476, Loss: 0.20449874225224174\n",
      "Validation Loss: 0.5363393796737804\n",
      "Epoch 477, Loss: 0.20493632901546566\n",
      "Validation Loss: 0.6307180205056834\n",
      "Epoch 478, Loss: 0.20448921880749768\n",
      "Validation Loss: 0.4356245242579039\n",
      "Epoch 479, Loss: 0.20471797770885533\n",
      "Validation Loss: 0.49731450302656305\n",
      "Epoch 480, Loss: 0.20438604814888434\n",
      "Validation Loss: 0.3881162599075672\n",
      "Epoch 481, Loss: 0.20594584465373394\n",
      "Validation Loss: 0.8745803528053816\n",
      "Epoch 482, Loss: 0.2035963507152574\n",
      "Validation Loss: 0.40859268501747487\n",
      "Epoch 483, Loss: 0.203306803588084\n",
      "Validation Loss: 0.5538882093374119\n",
      "Epoch 484, Loss: 0.2035935697210736\n",
      "Validation Loss: 0.4256886122531669\n",
      "Epoch 485, Loss: 0.20551689891794392\n",
      "Validation Loss: 0.5873132500537607\n",
      "Epoch 486, Loss: 0.2048062938157209\n",
      "Validation Loss: 0.4808609132156816\n",
      "Epoch 487, Loss: 0.20415393540332483\n",
      "Validation Loss: 0.5006893831630086\n",
      "Epoch 488, Loss: 0.20501550886953293\n",
      "Validation Loss: 0.38807964151681856\n",
      "Epoch 489, Loss: 0.2034501013627579\n",
      "Validation Loss: 0.391733166783355\n",
      "Epoch 490, Loss: 0.20371811009596946\n",
      "Validation Loss: 0.45949625726356064\n",
      "Epoch 491, Loss: 0.20286780799370865\n",
      "Validation Loss: 0.36487782590611034\n",
      "Epoch 492, Loss: 0.20318469606599834\n",
      "Validation Loss: 0.5154613521556521\n",
      "Epoch 493, Loss: 0.2027979441593553\n",
      "Validation Loss: 0.4200506931127504\n",
      "Epoch 494, Loss: 0.2036290804877184\n",
      "Validation Loss: 0.4904521613620048\n",
      "Epoch 495, Loss: 0.2050687828599367\n",
      "Validation Loss: 0.48056800594163496\n",
      "Epoch 496, Loss: 0.20447866311080234\n",
      "Validation Loss: 0.4659642386575078\n",
      "Epoch 497, Loss: 0.20471359518638185\n",
      "Validation Loss: 0.4268213950617369\n",
      "Epoch 498, Loss: 0.20331679784887752\n",
      "Validation Loss: 0.32972825075997864\n",
      "Epoch 499, Loss: 0.202286135990086\n",
      "Validation Loss: 0.4167245962592058\n",
      "Epoch 500, Loss: 0.2021885313050345\n",
      "Validation Loss: 0.4767205703050591\n",
      "Epoch 501, Loss: 0.20280850345138893\n",
      "Validation Loss: 0.45234733889269274\n",
      "Epoch 502, Loss: 0.2028926485976161\n",
      "Validation Loss: 0.4410973924190499\n",
      "Epoch 503, Loss: 0.2019967314852185\n",
      "Validation Loss: 0.49093928690566574\n",
      "Epoch 504, Loss: 0.2044987917422902\n",
      "Validation Loss: 0.49866954358511195\n",
      "Epoch 505, Loss: 0.20202403478757586\n",
      "Validation Loss: 0.3753186973721482\n",
      "Epoch 506, Loss: 0.20162623178560374\n",
      "Validation Loss: 0.35916611656200054\n",
      "Epoch 507, Loss: 0.20194996001068935\n",
      "Validation Loss: 0.3822232429371324\n",
      "Epoch 508, Loss: 0.20301275771804328\n",
      "Validation Loss: 0.40406610695428624\n",
      "Epoch 509, Loss: 0.2017648394631092\n",
      "Validation Loss: 0.4568513579839884\n",
      "Epoch 510, Loss: 0.20217508894153113\n",
      "Validation Loss: 0.4672569705303325\n",
      "Epoch 511, Loss: 0.20246550157059764\n",
      "Validation Loss: 0.3998232113067494\n",
      "Epoch 512, Loss: 0.2010756741646071\n",
      "Validation Loss: 0.3661016962209413\n",
      "Epoch 513, Loss: 0.20130686810630005\n",
      "Validation Loss: 0.33729769358801287\n",
      "Epoch 514, Loss: 0.20354081034053897\n",
      "Validation Loss: 0.47261473675106846\n",
      "Epoch 515, Loss: 0.20112597556828066\n",
      "Validation Loss: 0.4578593407952508\n",
      "Epoch 516, Loss: 0.20098983593891526\n",
      "Validation Loss: 0.5188711077667946\n",
      "Epoch 517, Loss: 0.20051846423641193\n",
      "Validation Loss: 0.37104671916296317\n",
      "Epoch 518, Loss: 0.20139721941289512\n",
      "Validation Loss: 0.5174561189704163\n",
      "Epoch 519, Loss: 0.20198840318724168\n",
      "Validation Loss: 0.4700490399155506\n",
      "Epoch 520, Loss: 0.20021469252140717\n",
      "Validation Loss: 0.39263846049475115\n",
      "Epoch 521, Loss: 0.20039711821131234\n",
      "Validation Loss: 0.3861762815436652\n",
      "Epoch 522, Loss: 0.20172616900053136\n",
      "Validation Loss: 0.35182540465232937\n",
      "Epoch 523, Loss: 0.20035757384327954\n",
      "Validation Loss: 0.3687525824751965\n",
      "Epoch 524, Loss: 0.2011226100067413\n",
      "Validation Loss: 0.40219687219969064\n",
      "Epoch 525, Loss: 0.20040200076737377\n",
      "Validation Loss: 0.32622208498245064\n",
      "Epoch 526, Loss: 0.20142549148565808\n",
      "Validation Loss: 0.41347807524509206\n",
      "Epoch 527, Loss: 0.20048203330140474\n",
      "Validation Loss: 0.5250958349815634\n",
      "Epoch 528, Loss: 0.20016749889784774\n",
      "Validation Loss: 0.4192665273020434\n",
      "Epoch 529, Loss: 0.20034836005246223\n",
      "Validation Loss: 0.38235831849796825\n",
      "Epoch 530, Loss: 0.20011090857604907\n",
      "Validation Loss: 0.4580599585937899\n",
      "Epoch 531, Loss: 0.20063245606197175\n",
      "Validation Loss: 0.4734031246152035\n",
      "Epoch 532, Loss: 0.20134238913915184\n",
      "Validation Loss: 0.4434814343965331\n",
      "Epoch 533, Loss: 0.20062617654370707\n",
      "Validation Loss: 0.4055370040411173\n",
      "Epoch 534, Loss: 0.20001009663263725\n",
      "Validation Loss: 0.4365005534748698\n",
      "Epoch 535, Loss: 0.2000293304364002\n",
      "Validation Loss: 0.4292176882194918\n",
      "Epoch 536, Loss: 0.19977135943292185\n",
      "Validation Loss: 0.41807849427988364\n",
      "Epoch 537, Loss: 0.19926101523776388\n",
      "Validation Loss: 0.3864028386251871\n",
      "Epoch 538, Loss: 0.1991271149626998\n",
      "Validation Loss: 0.4455919882585836\n",
      "Epoch 539, Loss: 0.20046860369485478\n",
      "Validation Loss: 0.46779368505921476\n",
      "Epoch 540, Loss: 0.19987380723360665\n",
      "Validation Loss: 0.4311844807031543\n",
      "Epoch 541, Loss: 0.1994063354335552\n",
      "Validation Loss: 0.4818675126447234\n",
      "Epoch 542, Loss: 0.19971822063590206\n",
      "Validation Loss: 0.3307816907763481\n",
      "Epoch 543, Loss: 0.1994802576468088\n",
      "Validation Loss: 0.4553248639716658\n",
      "Epoch 544, Loss: 0.19971359700917504\n",
      "Validation Loss: 0.4685655818429104\n",
      "Epoch 545, Loss: 0.19937514186685168\n",
      "Validation Loss: 0.5396886117929636\n",
      "Epoch 546, Loss: 0.19867601182831582\n",
      "Validation Loss: 0.4405590548764828\n",
      "Epoch 547, Loss: 0.20072768650255923\n",
      "Validation Loss: 0.40760171690652536\n",
      "Epoch 548, Loss: 0.19945678820963517\n",
      "Validation Loss: 0.4318518884653269\n",
      "Epoch 549, Loss: 0.19826491651406816\n",
      "Validation Loss: 0.4150098466942477\n",
      "Epoch 550, Loss: 0.1997081568507954\n",
      "Validation Loss: 0.3503186397774275\n",
      "Epoch 551, Loss: 0.19876643259424803\n",
      "Validation Loss: 0.37808712589186294\n",
      "Epoch 552, Loss: 0.19883756177110035\n",
      "Validation Loss: 0.34480875176052717\n",
      "Epoch 553, Loss: 0.199686371288154\n",
      "Validation Loss: 0.4606824825669444\n",
      "Epoch 554, Loss: 0.19994190278961216\n",
      "Validation Loss: 0.513055024105449\n",
      "Epoch 555, Loss: 0.19911511318195005\n",
      "Validation Loss: 0.4200559112914773\n",
      "Epoch 556, Loss: 0.19867028068577827\n",
      "Validation Loss: 0.39063425119533096\n",
      "Epoch 557, Loss: 0.20040468533718309\n",
      "Validation Loss: 0.41410528677840563\n",
      "Epoch 558, Loss: 0.2004821151928153\n",
      "Validation Loss: 0.45522592615249546\n",
      "Epoch 559, Loss: 0.19909895960847998\n",
      "Validation Loss: 0.6102418934189996\n",
      "Epoch 560, Loss: 0.19974208231149024\n",
      "Validation Loss: 0.7112566496050635\n",
      "Epoch 561, Loss: 0.19732862531185844\n",
      "Validation Loss: 0.44560499489307404\n",
      "Epoch 562, Loss: 0.19821876646994158\n",
      "Validation Loss: 0.3615215819242389\n",
      "Epoch 563, Loss: 0.19901138790991418\n",
      "Validation Loss: 0.4386547010305316\n",
      "Epoch 564, Loss: 0.1990095342747694\n",
      "Validation Loss: 0.5634266262830689\n",
      "Epoch 565, Loss: 0.19881525341161463\n",
      "Validation Loss: 0.5152832235014716\n",
      "Epoch 566, Loss: 0.2011925630314752\n",
      "Validation Loss: 0.7059053279632745\n",
      "Epoch 567, Loss: 0.20248646549014157\n",
      "Validation Loss: 0.8411194432613461\n",
      "Epoch 568, Loss: 0.20126955559867066\n",
      "Validation Loss: 0.3182032205337702\n",
      "Epoch 569, Loss: 0.19938787363123062\n",
      "Validation Loss: 0.40659225784068886\n",
      "Epoch 570, Loss: 0.19924985082430202\n",
      "Validation Loss: 0.37011662991934047\n",
      "Epoch 571, Loss: 0.19830194197941658\n",
      "Validation Loss: 0.6263516177964765\n",
      "Epoch 572, Loss: 0.1993144962200245\n",
      "Validation Loss: 0.4660033656414165\n",
      "Epoch 573, Loss: 0.19770035894890858\n",
      "Validation Loss: 0.3579315099605294\n",
      "Epoch 574, Loss: 0.19744158720294402\n",
      "Validation Loss: 0.4221224375935488\n",
      "Epoch 575, Loss: 0.19791792564873778\n",
      "Validation Loss: 0.47716334150281065\n",
      "Epoch 576, Loss: 0.1996848614626499\n",
      "Validation Loss: 0.6421014396257179\n",
      "Epoch 577, Loss: 0.19742414963886487\n",
      "Validation Loss: 0.29030904069889424\n",
      "Epoch 578, Loss: 0.1974282919667488\n",
      "Validation Loss: 0.8060018309327059\n",
      "Epoch 579, Loss: 0.19789137258086092\n",
      "Validation Loss: 0.9084166926006938\n",
      "Epoch 580, Loss: 0.19990268388633117\n",
      "Validation Loss: 0.4449787631977436\n",
      "Epoch 581, Loss: 0.1971100618369704\n",
      "Validation Loss: 0.4116706394178923\n",
      "Epoch 582, Loss: 0.1990923994805577\n",
      "Validation Loss: 0.6563909545887349\n",
      "Epoch 583, Loss: 0.19775245652728995\n",
      "Validation Loss: 0.3877372603083766\n",
      "Epoch 584, Loss: 0.1982476756756389\n",
      "Validation Loss: 0.42120053289934645\n",
      "Epoch 585, Loss: 0.19711527817471083\n",
      "Validation Loss: 0.4475424428318822\n",
      "Epoch 586, Loss: 0.19689329046496126\n",
      "Validation Loss: 0.5461514148601266\n",
      "Epoch 587, Loss: 0.19638972751103168\n",
      "Validation Loss: 0.4806828242401744\n",
      "Epoch 588, Loss: 0.19776638304858013\n",
      "Validation Loss: 0.7433492887851804\n",
      "Epoch 589, Loss: 0.19873200151202983\n",
      "Validation Loss: 0.6626282864531805\n",
      "Epoch 590, Loss: 0.19804753718334575\n",
      "Validation Loss: 0.636269309839537\n",
      "Epoch 591, Loss: 0.19703587142447399\n",
      "Validation Loss: 0.277842293991599\n",
      "Epoch 592, Loss: 0.1970344083600266\n",
      "Validation Loss: 0.43213256431180375\n",
      "Epoch 593, Loss: 0.19747916256012613\n",
      "Validation Loss: 0.5621008072481599\n",
      "Epoch 594, Loss: 0.196369141838405\n",
      "Validation Loss: 0.5938849851142528\n",
      "Epoch 595, Loss: 0.19806830740945283\n",
      "Validation Loss: 0.5319316028509029\n",
      "Epoch 596, Loss: 0.19688085241373196\n",
      "Validation Loss: 0.43045195902502814\n",
      "Epoch 597, Loss: 0.19562526406763597\n",
      "Validation Loss: 0.3891827949257784\n",
      "Epoch 598, Loss: 0.1960077685368962\n",
      "Validation Loss: 0.5641011453645174\n",
      "Epoch 599, Loss: 0.19523558382291434\n",
      "Validation Loss: 0.35526930038319077\n",
      "Epoch 600, Loss: 0.1957230282687517\n",
      "Validation Loss: 0.45728444186754\n",
      "Epoch 601, Loss: 0.1964318949815839\n",
      "Validation Loss: 0.4889535488084305\n",
      "Epoch 602, Loss: 0.19542269815885743\n",
      "Validation Loss: 0.44431557991476944\n",
      "Epoch 603, Loss: 0.19534045826037263\n",
      "Validation Loss: 0.8097489545511645\n",
      "Epoch 604, Loss: 0.19405828050316073\n",
      "Validation Loss: 0.5288355925748515\n",
      "Epoch 605, Loss: 0.19563908983281877\n",
      "Validation Loss: 0.29923747895761976\n",
      "Epoch 606, Loss: 0.19463133194678744\n",
      "Validation Loss: 0.5239193945430046\n",
      "Epoch 607, Loss: 0.19383549315551685\n",
      "Validation Loss: 0.4767155058162157\n",
      "Epoch 608, Loss: 0.19385665436383598\n",
      "Validation Loss: 0.2810487651894259\n",
      "Epoch 609, Loss: 0.19419267297137616\n",
      "Validation Loss: 0.3739292902308841\n",
      "Epoch 610, Loss: 0.19336367987616118\n",
      "Validation Loss: 0.42687627084033436\n",
      "Epoch 611, Loss: 0.19398469895904147\n",
      "Validation Loss: 0.5584997721882754\n",
      "Epoch 612, Loss: 0.19415074827279463\n",
      "Validation Loss: 0.43373415359230927\n",
      "Epoch 613, Loss: 0.19506362019929774\n",
      "Validation Loss: 0.4572670858266742\n",
      "Epoch 614, Loss: 0.19314872341360464\n",
      "Validation Loss: 0.4883489452822264\n",
      "Epoch 615, Loss: 0.1933865921225312\n",
      "Validation Loss: 0.44676283764284713\n",
      "Epoch 616, Loss: 0.19353794962774182\n",
      "Validation Loss: 0.2803089509523192\n",
      "Epoch 617, Loss: 0.19266308395755152\n",
      "Validation Loss: 0.6862038644247277\n",
      "Epoch 618, Loss: 0.19463080150443454\n",
      "Validation Loss: 0.43382151840731153\n",
      "Epoch 619, Loss: 0.1934129752565262\n",
      "Validation Loss: 0.40137923110363094\n",
      "Epoch 620, Loss: 0.1934722472025558\n",
      "Validation Loss: 0.40781915326451146\n",
      "Epoch 621, Loss: 0.19402780288527177\n",
      "Validation Loss: 0.526384349132693\n",
      "Epoch 622, Loss: 0.19393136473589165\n",
      "Validation Loss: 0.4946965408879657\n",
      "Epoch 623, Loss: 0.19307204888223908\n",
      "Validation Loss: 0.3111227917116742\n",
      "Epoch 624, Loss: 0.1932407299403188\n",
      "Validation Loss: 0.6838069759136023\n",
      "Epoch 625, Loss: 0.1940239924331044\n",
      "Validation Loss: 0.7100085512150166\n",
      "Epoch 626, Loss: 0.19313079240017159\n",
      "Validation Loss: 0.593697520189507\n",
      "Epoch 627, Loss: 0.1930606200164834\n",
      "Validation Loss: 0.8387674945731496\n",
      "Epoch 628, Loss: 0.19245072553843953\n",
      "Validation Loss: 0.44153512737085654\n",
      "Epoch 629, Loss: 0.19290641203609316\n",
      "Validation Loss: 0.38021028111147326\n",
      "Epoch 630, Loss: 0.19357661701478932\n",
      "Validation Loss: 0.6176785445490549\n",
      "Epoch 631, Loss: 0.19338057508562193\n",
      "Validation Loss: 0.47546882580879124\n",
      "Epoch 632, Loss: 0.19319796079206605\n",
      "Validation Loss: 0.4137527956519016\n",
      "Epoch 633, Loss: 0.19326699421156285\n",
      "Validation Loss: 1.1769053409265917\n",
      "Epoch 634, Loss: 0.19479610422236282\n",
      "Validation Loss: 0.4165645820456882\n",
      "Epoch 635, Loss: 0.19458475440394046\n",
      "Validation Loss: 0.8849207183649374\n",
      "Epoch 636, Loss: 0.19353548387542022\n",
      "Validation Loss: 0.6238138661828152\n",
      "Epoch 637, Loss: 0.1938767651702429\n",
      "Validation Loss: 0.43552466291327807\n",
      "Epoch 638, Loss: 0.19386720107218555\n",
      "Validation Loss: 0.6386468195637991\n",
      "Epoch 639, Loss: 0.1951763291171817\n",
      "Validation Loss: 0.8336569708447124\n",
      "Epoch 640, Loss: 0.19273126025705836\n",
      "Validation Loss: 0.5426438572101815\n",
      "Epoch 641, Loss: 0.19232354752806036\n",
      "Validation Loss: 0.691276571778364\n",
      "Epoch 642, Loss: 0.19427165651130815\n",
      "Validation Loss: 1.0954590938812079\n",
      "Epoch 643, Loss: 0.19497763525781242\n",
      "Validation Loss: 0.9565053298029789\n",
      "Epoch 644, Loss: 0.19293160606608833\n",
      "Validation Loss: 0.3937221290760262\n",
      "Epoch 645, Loss: 0.19171725763657757\n",
      "Validation Loss: 0.992005004439243\n",
      "Epoch 646, Loss: 0.19208836035673008\n",
      "Validation Loss: 0.6495723686245984\n",
      "Epoch 647, Loss: 0.19211201029721386\n",
      "Validation Loss: 0.6124112848625627\n",
      "Epoch 648, Loss: 0.19224081431017365\n",
      "Validation Loss: 0.5717663511980412\n",
      "Epoch 649, Loss: 0.1915484111712769\n",
      "Validation Loss: 0.9724431744841642\n",
      "Epoch 650, Loss: 0.19225777880570225\n",
      "Validation Loss: 0.6273995807697607\n",
      "Epoch 651, Loss: 0.1921403231749008\n",
      "Validation Loss: 0.9176066559414531\n",
      "Epoch 652, Loss: 0.19222247020103211\n",
      "Validation Loss: 0.941552551679833\n",
      "Epoch 653, Loss: 0.19360425864714523\n",
      "Validation Loss: 0.6607199290464091\n",
      "Epoch 654, Loss: 0.19191199701366035\n",
      "Validation Loss: 0.5611479784860167\n",
      "Epoch 655, Loss: 0.19112419443161682\n",
      "Validation Loss: 0.6741505833559258\n",
      "Epoch 656, Loss: 0.1917752133639053\n",
      "Validation Loss: 0.708194061074146\n",
      "Epoch 657, Loss: 0.19144967527583587\n",
      "Validation Loss: 0.6228571208410485\n",
      "Epoch 658, Loss: 0.19166174234259267\n",
      "Validation Loss: 0.8000262344992438\n",
      "Epoch 659, Loss: 0.1922259498604162\n",
      "Validation Loss: 0.824567259051079\n",
      "Epoch 660, Loss: 0.19174519668571477\n",
      "Validation Loss: 0.7123100774925809\n",
      "Epoch 661, Loss: 0.19175677213731201\n",
      "Validation Loss: 0.7635080952977025\n",
      "Epoch 662, Loss: 0.1921387829406317\n",
      "Validation Loss: 0.5285112982572511\n",
      "Epoch 663, Loss: 0.19169228991796805\n",
      "Validation Loss: 1.0785712994808374\n",
      "Epoch 664, Loss: 0.19095382642347453\n",
      "Validation Loss: 1.0541738916275114\n",
      "Epoch 665, Loss: 0.19175978492252355\n",
      "Validation Loss: 0.3852815801321074\n",
      "Epoch 666, Loss: 0.19093349692953188\n",
      "Validation Loss: 0.6329706134491189\n",
      "Epoch 667, Loss: 0.19351805445499892\n",
      "Validation Loss: 0.7169272393681282\n",
      "Epoch 668, Loss: 0.19116097706001858\n",
      "Validation Loss: 0.7220372730909392\n",
      "Epoch 669, Loss: 0.19144511276986018\n",
      "Validation Loss: 1.093374420044034\n",
      "Epoch 670, Loss: 0.19133143354380547\n",
      "Validation Loss: 0.8499656194864318\n",
      "Epoch 671, Loss: 0.19074179483361023\n",
      "Validation Loss: 0.5733775475690531\n",
      "Epoch 672, Loss: 0.19062562778505476\n",
      "Validation Loss: 0.6770079000744709\n",
      "Epoch 673, Loss: 0.190533367570403\n",
      "Validation Loss: 0.818481081446936\n",
      "Epoch 674, Loss: 0.19191252812743187\n",
      "Validation Loss: 0.8281158354393271\n",
      "Epoch 675, Loss: 0.19200213716993497\n",
      "Validation Loss: 0.7049265101898549\n",
      "Epoch 676, Loss: 0.19035997261228257\n",
      "Validation Loss: 1.0825437605381012\n",
      "Epoch 677, Loss: 0.1910317042669238\n",
      "Validation Loss: 0.5694146211757216\n",
      "Epoch 678, Loss: 0.19140671130789574\n",
      "Validation Loss: 0.9285359944021979\n",
      "Epoch 679, Loss: 0.19062739695140787\n",
      "Validation Loss: 0.7278854424177215\n",
      "Epoch 680, Loss: 0.19078152174086765\n",
      "Validation Loss: 0.47914370201354806\n",
      "Epoch 681, Loss: 0.18979451885490223\n",
      "Validation Loss: 0.650772436413654\n",
      "Epoch 682, Loss: 0.18937959636799817\n",
      "Validation Loss: 0.47825467933055965\n",
      "Epoch 683, Loss: 0.1900298197775386\n",
      "Validation Loss: 0.7292360219844553\n",
      "Epoch 684, Loss: 0.1894592097418946\n",
      "Validation Loss: 0.5429505411275598\n",
      "Epoch 685, Loss: 0.1899007649139263\n",
      "Validation Loss: 0.78657968654189\n",
      "Epoch 686, Loss: 0.19123015689208758\n",
      "Validation Loss: 0.6916044423746508\n",
      "Epoch 687, Loss: 0.1905196767387002\n",
      "Validation Loss: 0.7660359719464945\n",
      "Epoch 688, Loss: 0.19077279394882363\n",
      "Validation Loss: 1.5211603419725286\n",
      "Epoch 689, Loss: 0.19119679518477167\n",
      "Validation Loss: 0.8707494035709736\n",
      "Epoch 690, Loss: 0.19169043305568223\n",
      "Validation Loss: 0.5458957653406055\n",
      "Epoch 691, Loss: 0.19009204048576744\n",
      "Validation Loss: 0.7168221154878306\n",
      "Epoch 692, Loss: 0.19090946980340537\n",
      "Validation Loss: 1.214875301649404\n",
      "Epoch 693, Loss: 0.19026409438269776\n",
      "Validation Loss: 1.004105659418328\n",
      "Epoch 694, Loss: 0.19041846092617096\n",
      "Validation Loss: 1.2838961509771125\n",
      "Epoch 695, Loss: 0.1897990043426669\n",
      "Validation Loss: 0.7892857672170152\n",
      "Epoch 696, Loss: 0.19020777256336324\n",
      "Validation Loss: 0.7559759187143903\n",
      "Epoch 697, Loss: 0.1892580446938789\n",
      "Validation Loss: 0.8295236053161843\n",
      "Epoch 698, Loss: 0.189020563826658\n",
      "Validation Loss: 0.7927811700244283\n",
      "Epoch 699, Loss: 0.1902885885562661\n",
      "Validation Loss: 0.7473299596891847\n",
      "Epoch 700, Loss: 0.19030464089713817\n",
      "Validation Loss: 0.8683980315230614\n",
      "Epoch 701, Loss: 0.1895147304722043\n",
      "Validation Loss: 0.7784223854541779\n",
      "Epoch 702, Loss: 0.18926842629822882\n",
      "Validation Loss: 0.9857327931149061\n",
      "Epoch 703, Loss: 0.1896730324816565\n",
      "Validation Loss: 0.9250750243663788\n",
      "Epoch 704, Loss: 0.18959323866942593\n",
      "Validation Loss: 0.9065796365571577\n",
      "Epoch 705, Loss: 0.1891161931332114\n",
      "Validation Loss: 0.9342198898625929\n",
      "Epoch 706, Loss: 0.1883201424985431\n",
      "Validation Loss: 0.7798991362715877\n",
      "Epoch 707, Loss: 0.18954028676502233\n",
      "Validation Loss: 0.8568608622218288\n",
      "Epoch 708, Loss: 0.18933103677578444\n",
      "Validation Loss: 0.8029320697451747\n",
      "Epoch 709, Loss: 0.18870623845087234\n",
      "Validation Loss: 0.6981641161580419\n",
      "Epoch 710, Loss: 0.18909623881065568\n",
      "Validation Loss: 1.1539175399514132\n",
      "Epoch 711, Loss: 0.19051276259990627\n",
      "Validation Loss: 0.8323649500691613\n",
      "Epoch 712, Loss: 0.18943830867579511\n",
      "Validation Loss: 1.138853208963261\n",
      "Epoch 713, Loss: 0.18864455463927846\n",
      "Validation Loss: 0.5542969249708708\n",
      "Epoch 714, Loss: 0.18865980201422475\n",
      "Validation Loss: 0.9615715839142023\n",
      "Epoch 715, Loss: 0.18816825359799835\n",
      "Validation Loss: 0.6638412669647572\n",
      "Epoch 716, Loss: 0.18858423413232314\n",
      "Validation Loss: 0.70221441637638\n",
      "Epoch 717, Loss: 0.18863007831278927\n",
      "Validation Loss: 0.7669422577979953\n",
      "Epoch 718, Loss: 0.18813151773065329\n",
      "Validation Loss: 0.8130736839632655\n",
      "Epoch 719, Loss: 0.18876199422100948\n",
      "Validation Loss: 0.8306622955688211\n",
      "Epoch 720, Loss: 0.18889490417529678\n",
      "Validation Loss: 0.7446318150952805\n",
      "Epoch 721, Loss: 0.18854632022855586\n",
      "Validation Loss: 0.9771447458932566\n",
      "Epoch 722, Loss: 0.18812919442736825\n",
      "Validation Loss: 1.0424210574737816\n",
      "Epoch 723, Loss: 0.18909000873912213\n",
      "Validation Loss: 0.9229977602182433\n",
      "Epoch 724, Loss: 0.18813844566601653\n",
      "Validation Loss: 0.6951317156470099\n",
      "Epoch 725, Loss: 0.1881020190844009\n",
      "Validation Loss: 0.8456529468991035\n",
      "Epoch 726, Loss: 0.1885073025126097\n",
      "Validation Loss: 1.3655897087829059\n",
      "Epoch 727, Loss: 0.18938338262743729\n",
      "Validation Loss: 0.8947621632454007\n",
      "Epoch 728, Loss: 0.19010133950342967\n",
      "Validation Loss: 1.9828002203342527\n",
      "Epoch 729, Loss: 0.18844825038036636\n",
      "Validation Loss: 0.7993535104879114\n",
      "Epoch 730, Loss: 0.18892075113822207\n",
      "Validation Loss: 1.71618829217068\n",
      "Epoch 731, Loss: 0.18785978223435407\n",
      "Validation Loss: 1.2683908301730489\n",
      "Epoch 732, Loss: 0.1887296182428335\n",
      "Validation Loss: 1.0705456768357478\n",
      "Epoch 733, Loss: 0.1881608961496589\n",
      "Validation Loss: 1.025121310422587\n",
      "Epoch 734, Loss: 0.18829314355456897\n",
      "Validation Loss: 0.7617977826401244\n",
      "Epoch 735, Loss: 0.18847400922501503\n",
      "Validation Loss: 1.1326987854270048\n",
      "Epoch 736, Loss: 0.18900729369285496\n",
      "Validation Loss: 0.9719944818075313\n",
      "Epoch 737, Loss: 0.1872387807166507\n",
      "Validation Loss: 1.2246405010999635\n",
      "Epoch 738, Loss: 0.18805948622134883\n",
      "Validation Loss: 1.6097989387290423\n",
      "Epoch 739, Loss: 0.1886219750591662\n",
      "Validation Loss: 1.2876759213070537\n",
      "Epoch 740, Loss: 0.1888356286555875\n",
      "Validation Loss: 1.759213697078616\n",
      "Epoch 741, Loss: 0.18803587527729052\n",
      "Validation Loss: 1.1134478560713834\n",
      "Epoch 742, Loss: 0.1885564432934273\n",
      "Validation Loss: 0.6616608510876811\n",
      "Epoch 743, Loss: 0.18761788997365986\n",
      "Validation Loss: 1.2880173228507819\n",
      "Epoch 744, Loss: 0.18811149321236584\n",
      "Validation Loss: 1.9254058433133503\n",
      "Epoch 745, Loss: 0.19032983633494654\n",
      "Validation Loss: 2.365707572116408\n",
      "Epoch 746, Loss: 0.18980440272148266\n",
      "Validation Loss: 1.412219901417577\n",
      "Epoch 747, Loss: 0.18848259255376665\n",
      "Validation Loss: 1.4339996409970661\n",
      "Epoch 748, Loss: 0.18816375719426676\n",
      "Validation Loss: 1.1915326187776965\n",
      "Epoch 749, Loss: 0.18935207056618014\n",
      "Validation Loss: 1.384669660135757\n",
      "Epoch 750, Loss: 0.189582243994918\n",
      "Validation Loss: 2.4459324908810993\n",
      "Epoch 751, Loss: 0.18782021705234467\n",
      "Validation Loss: 1.803826854672543\n",
      "Epoch 752, Loss: 0.18800439811203368\n",
      "Validation Loss: 1.5060213457706362\n",
      "Epoch 753, Loss: 0.1895593285430656\n",
      "Validation Loss: 1.5729413573132005\n",
      "Epoch 754, Loss: 0.18724344694597084\n",
      "Validation Loss: 1.2723711687465047\n",
      "Epoch 755, Loss: 0.18705491526702114\n",
      "Validation Loss: 1.1982063935246579\n",
      "Epoch 756, Loss: 0.18702512696861875\n",
      "Validation Loss: 1.2442218592000562\n",
      "Epoch 757, Loss: 0.18754593739935824\n",
      "Validation Loss: 1.7692140576451323\n",
      "Epoch 758, Loss: 0.18738373308334239\n",
      "Validation Loss: 1.5022521573443746\n",
      "Epoch 759, Loss: 0.18731390970737435\n",
      "Validation Loss: 1.468302450900854\n",
      "Epoch 760, Loss: 0.18726980703514676\n",
      "Validation Loss: 3.3363974482514136\n",
      "Epoch 761, Loss: 0.18973061509517042\n",
      "Validation Loss: 1.8956361149632655\n",
      "Epoch 762, Loss: 0.1875270003192993\n",
      "Validation Loss: 1.9754017480584078\n",
      "Epoch 763, Loss: 0.18900243342355932\n",
      "Validation Loss: 1.4347991264143656\n",
      "Epoch 764, Loss: 0.1917053469738295\n",
      "Validation Loss: 1.1612495932468148\n",
      "Epoch 765, Loss: 0.18693275084762379\n",
      "Validation Loss: 1.9558757931687112\n",
      "Epoch 766, Loss: 0.18824796259489862\n",
      "Validation Loss: 1.5630767054336017\n",
      "Epoch 767, Loss: 0.19087562664563573\n",
      "Validation Loss: 5.007400673489238\n",
      "Epoch 768, Loss: 0.18994071341098048\n",
      "Validation Loss: 2.5126808812451915\n",
      "Epoch 769, Loss: 0.18911922904987669\n",
      "Validation Loss: 1.502189851084421\n",
      "Epoch 770, Loss: 0.1877572411281425\n",
      "Validation Loss: 0.8970373356065084\n",
      "Epoch 771, Loss: 0.1872039740948483\n",
      "Validation Loss: 1.3922198428664097\n",
      "Epoch 772, Loss: 0.18774978625913\n",
      "Validation Loss: 1.5850578102954598\n",
      "Epoch 773, Loss: 0.189356513754573\n",
      "Validation Loss: 1.5694222713625707\n",
      "Epoch 774, Loss: 0.18775642258223407\n",
      "Validation Loss: 1.8046997397444968\n",
      "Epoch 775, Loss: 0.19149902690375267\n",
      "Validation Loss: 0.7475400130416072\n",
      "Epoch 776, Loss: 0.18828444946340697\n",
      "Validation Loss: 1.042040422905323\n",
      "Epoch 777, Loss: 0.1876690313869784\n",
      "Validation Loss: 1.3566944772420928\n",
      "Epoch 778, Loss: 0.18726368395741597\n",
      "Validation Loss: 1.9131972581841226\n",
      "Epoch 779, Loss: 0.1879355881774668\n",
      "Validation Loss: 3.1285838526348737\n",
      "Epoch 780, Loss: 0.1877675843749975\n",
      "Validation Loss: 1.3499982509502144\n",
      "Epoch 781, Loss: 0.1872376673313421\n",
      "Validation Loss: 1.1132750330969345\n",
      "Epoch 782, Loss: 0.1871178886571596\n",
      "Validation Loss: 1.6790603870569274\n",
      "Epoch 783, Loss: 0.18712798919701992\n",
      "Validation Loss: 1.7130657891894496\n",
      "Epoch 784, Loss: 0.18790212563823822\n",
      "Validation Loss: 1.777411672958108\n",
      "Epoch 785, Loss: 0.18721561261647662\n",
      "Validation Loss: 3.7536872375843138\n",
      "Epoch 786, Loss: 0.18713052074836437\n",
      "Validation Loss: 2.057711881260539\n",
      "Epoch 787, Loss: 0.1886990511530014\n",
      "Validation Loss: 1.8720405087914578\n",
      "Epoch 788, Loss: 0.18729967238424822\n",
      "Validation Loss: 1.6486549640810766\n",
      "Epoch 789, Loss: 0.18721994668851757\n",
      "Validation Loss: 2.2145401572072227\n",
      "Epoch 790, Loss: 0.1874978805480655\n",
      "Validation Loss: 1.6566316291343335\n",
      "Epoch 791, Loss: 0.1890061117578731\n",
      "Validation Loss: 2.665414050567982\n",
      "Epoch 792, Loss: 0.1866859501379347\n",
      "Validation Loss: 1.91824273037356\n",
      "Epoch 793, Loss: 0.18747527082992155\n",
      "Validation Loss: 2.4920589424843014\n",
      "Epoch 794, Loss: 0.1865979229901419\n",
      "Validation Loss: 1.7740764271381289\n",
      "Epoch 795, Loss: 0.18721745078733495\n",
      "Validation Loss: 1.3876259403173314\n",
      "Epoch 796, Loss: 0.1883876256558091\n",
      "Validation Loss: 2.0352752014648083\n",
      "Epoch 797, Loss: 0.18849053090914739\n",
      "Validation Loss: 2.570008302843848\n",
      "Epoch 798, Loss: 0.1888739429955739\n",
      "Validation Loss: 2.9601341000823087\n",
      "Epoch 799, Loss: 0.18809370396572145\n",
      "Validation Loss: 1.6816849930341853\n",
      "Epoch 800, Loss: 0.18602590612677294\n",
      "Validation Loss: 2.5138103518375132\n",
      "Epoch 801, Loss: 0.18588052124738\n",
      "Validation Loss: 2.2359597322552704\n",
      "Epoch 802, Loss: 0.18767444544666728\n",
      "Validation Loss: 2.706014949221944\n",
      "Epoch 803, Loss: 0.18721194002170896\n",
      "Validation Loss: 2.273116158884625\n",
      "Epoch 804, Loss: 0.18788027462311263\n",
      "Validation Loss: 1.6243075894754986\n",
      "Epoch 805, Loss: 0.18662625020586474\n",
      "Validation Loss: 1.9822142512299294\n",
      "Epoch 806, Loss: 0.1858814796216266\n",
      "Validation Loss: 1.5071051037588785\n",
      "Epoch 807, Loss: 0.18570751166187746\n",
      "Validation Loss: 1.6708481727644455\n",
      "Epoch 808, Loss: 0.1868263491710951\n",
      "Validation Loss: 2.0960351450498713\n",
      "Epoch 809, Loss: 0.18668892539951\n",
      "Validation Loss: 1.4544615794059843\n",
      "Epoch 810, Loss: 0.18696599165627428\n",
      "Validation Loss: 2.6015182678089586\n",
      "Epoch 811, Loss: 0.18526070478350617\n",
      "Validation Loss: 1.1334738343261008\n",
      "Epoch 812, Loss: 0.18506751209497452\n",
      "Validation Loss: 1.1688798305600188\n",
      "Epoch 813, Loss: 0.18564231874594508\n",
      "Validation Loss: 2.364697973395503\n",
      "Epoch 814, Loss: 0.18794549486145032\n",
      "Validation Loss: 2.288986781308817\n",
      "Epoch 815, Loss: 0.18451356843417122\n",
      "Validation Loss: 1.5634104720381803\n",
      "Epoch 816, Loss: 0.18607393658698299\n",
      "Validation Loss: 2.8541384685871214\n",
      "Epoch 817, Loss: 0.18552448544218098\n",
      "Validation Loss: 1.6154921983563624\n",
      "Epoch 818, Loss: 0.1850442446681649\n",
      "Validation Loss: 2.2878155070681903\n",
      "Epoch 819, Loss: 0.1848408034136302\n",
      "Validation Loss: 0.9126650067262871\n",
      "Epoch 820, Loss: 0.1861094618216157\n",
      "Validation Loss: 2.004797098248504\n",
      "Epoch 821, Loss: 0.18653097620970288\n",
      "Validation Loss: 2.124778163987537\n",
      "Epoch 822, Loss: 0.1848982124437773\n",
      "Validation Loss: 2.058867691561233\n",
      "Epoch 823, Loss: 0.1864381664064388\n",
      "Validation Loss: 1.3542587022448695\n",
      "Epoch 824, Loss: 0.18435419040060666\n",
      "Validation Loss: 1.8793933654940405\n",
      "Epoch 825, Loss: 0.18517359363478283\n",
      "Validation Loss: 1.250922441482544\n",
      "Epoch 826, Loss: 0.1841241654961608\n",
      "Validation Loss: 1.1270258024681445\n",
      "Epoch 827, Loss: 0.1848278727360763\n",
      "Validation Loss: 1.5977135099643884\n",
      "Epoch 828, Loss: 0.18499880806044783\n",
      "Validation Loss: 1.968713912852975\n",
      "Epoch 829, Loss: 0.1858321014401871\n",
      "Validation Loss: 1.0684886165829592\n",
      "Epoch 830, Loss: 0.18518576231720144\n",
      "Validation Loss: 1.2173485554928003\n",
      "Epoch 831, Loss: 0.18447306513959585\n",
      "Validation Loss: 1.5967191208240599\n",
      "Epoch 832, Loss: 0.18426956595896288\n",
      "Validation Loss: 1.5420078538184943\n",
      "Epoch 833, Loss: 0.18460013340552187\n",
      "Validation Loss: 1.4472255512725476\n",
      "Epoch 834, Loss: 0.18597292202676452\n",
      "Validation Loss: 1.691571684770806\n",
      "Epoch 835, Loss: 0.18873879565792376\n",
      "Validation Loss: 3.3999591871749524\n",
      "Epoch 836, Loss: 0.1884042503702086\n",
      "Validation Loss: 1.5109146528465802\n",
      "Epoch 837, Loss: 0.18686402480789396\n",
      "Validation Loss: 1.5095129955646605\n",
      "Epoch 838, Loss: 0.18599602495601705\n",
      "Validation Loss: 6.745127672372862\n",
      "Epoch 839, Loss: 0.18520323360382124\n",
      "Validation Loss: 0.9439038135284601\n",
      "Epoch 840, Loss: 0.1869660428790159\n",
      "Validation Loss: 0.9049324330895446\n",
      "Epoch 841, Loss: 0.1863543642901404\n",
      "Validation Loss: 0.8783050210669984\n",
      "Epoch 842, Loss: 0.18542472955359277\n",
      "Validation Loss: 1.5591480808202611\n",
      "Epoch 843, Loss: 0.18480091929695633\n",
      "Validation Loss: 1.598205616307813\n",
      "Epoch 844, Loss: 0.1858447974461109\n",
      "Validation Loss: 1.3376246288765308\n",
      "Epoch 845, Loss: 0.1860448647355444\n",
      "Validation Loss: 1.2858954085860141\n",
      "Epoch 846, Loss: 0.18468228721081517\n",
      "Validation Loss: 1.3762436245763026\n",
      "Epoch 847, Loss: 0.18479941893628862\n",
      "Validation Loss: 1.5757003107736276\n",
      "Epoch 848, Loss: 0.18552370522127942\n",
      "Validation Loss: 1.0684291318405505\n",
      "Epoch 849, Loss: 0.18403042078581314\n",
      "Validation Loss: 1.1454121345697448\n",
      "Epoch 850, Loss: 0.18375482291069833\n",
      "Validation Loss: 1.0326918183371079\n",
      "Epoch 851, Loss: 0.18311859496197727\n",
      "Validation Loss: 1.275127151677775\n",
      "Epoch 852, Loss: 0.18459309858464917\n",
      "Validation Loss: 1.328111142613167\n",
      "Epoch 853, Loss: 0.18524298398907102\n",
      "Validation Loss: 0.9952877177748569\n",
      "Epoch 854, Loss: 0.18477074339509356\n",
      "Validation Loss: 1.4734317771224088\n",
      "Epoch 855, Loss: 0.18420214639153593\n",
      "Validation Loss: 0.9879469982413358\n",
      "Epoch 856, Loss: 0.1846307326584708\n",
      "Validation Loss: 1.0467516835345778\n",
      "Epoch 857, Loss: 0.1832811830911872\n",
      "Validation Loss: 0.485471056297768\n",
      "Epoch 858, Loss: 0.18384067848498045\n",
      "Validation Loss: 0.7689961357865223\n",
      "Epoch 859, Loss: 0.18441234589662664\n",
      "Validation Loss: 1.471650062605392\n",
      "Epoch 860, Loss: 0.18339672418267922\n",
      "Validation Loss: 1.4208283355069715\n",
      "Epoch 861, Loss: 0.18315227678435486\n",
      "Validation Loss: 0.8696272996969001\n",
      "Epoch 862, Loss: 0.18850784070876447\n",
      "Validation Loss: 1.136063850896303\n",
      "Epoch 863, Loss: 0.18376469183280025\n",
      "Validation Loss: 1.7226362048193467\n",
      "Epoch 864, Loss: 0.18377399472736342\n",
      "Validation Loss: 1.0760756731033325\n",
      "Epoch 865, Loss: 0.1835012358375067\n",
      "Validation Loss: 1.0608436964278998\n",
      "Epoch 866, Loss: 0.18618342538125987\n",
      "Validation Loss: 1.6199234918106433\n",
      "Epoch 867, Loss: 0.18304177832811377\n",
      "Validation Loss: 0.9665949864442959\n",
      "Epoch 868, Loss: 0.1838927963900185\n",
      "Validation Loss: 2.1201631080272585\n",
      "Epoch 869, Loss: 0.18427330608544654\n",
      "Validation Loss: 1.5997432629729427\n",
      "Epoch 870, Loss: 0.1832235045514481\n",
      "Validation Loss: 1.1331614657889966\n",
      "Epoch 871, Loss: 0.1833414062641041\n",
      "Validation Loss: 1.1054285986478938\n",
      "Epoch 872, Loss: 0.18276594734079268\n",
      "Validation Loss: 1.5070273196974466\n",
      "Epoch 873, Loss: 0.1823732647503358\n",
      "Validation Loss: 1.465905861799107\n",
      "Epoch 874, Loss: 0.18240703638036584\n",
      "Validation Loss: 1.0003709363382915\n",
      "Epoch 875, Loss: 0.18174524765548317\n",
      "Validation Loss: 1.3817368440849835\n",
      "Epoch 876, Loss: 0.1830130574011872\n",
      "Validation Loss: 1.2851886451244354\n",
      "Epoch 877, Loss: 0.1822395557537675\n",
      "Validation Loss: 2.1207091392472734\n",
      "Epoch 878, Loss: 0.1832456673213909\n",
      "Validation Loss: 1.220284761384476\n",
      "Epoch 879, Loss: 0.18202063544155206\n",
      "Validation Loss: 1.3446193171101948\n",
      "Epoch 880, Loss: 0.18204059981372814\n",
      "Validation Loss: 1.3249441024868986\n",
      "Epoch 881, Loss: 0.18232087624194318\n",
      "Validation Loss: 1.2838216070518937\n",
      "Epoch 882, Loss: 0.18347846949473023\n",
      "Validation Loss: 1.366196357926657\n",
      "Epoch 883, Loss: 0.1818113103725536\n",
      "Validation Loss: 1.217861331479494\n",
      "Epoch 884, Loss: 0.1831663187246683\n",
      "Validation Loss: 1.3606964571531428\n",
      "Epoch 885, Loss: 0.18300942382450366\n",
      "Validation Loss: 1.005757747001426\n",
      "Epoch 886, Loss: 0.18211412900885524\n",
      "Validation Loss: 0.7868793156257895\n",
      "Epoch 887, Loss: 0.18245061009385904\n",
      "Validation Loss: 0.9873930326727933\n",
      "Epoch 888, Loss: 0.18180904909968376\n",
      "Validation Loss: 1.0415647889292516\n",
      "Epoch 889, Loss: 0.18157698068941056\n",
      "Validation Loss: 1.0112031933873198\n",
      "Epoch 890, Loss: 0.18222604787280394\n",
      "Validation Loss: 1.473740842453269\n",
      "Epoch 891, Loss: 0.1812313173919223\n",
      "Validation Loss: 1.0261800088161646\n",
      "Epoch 892, Loss: 0.18118900437514449\n",
      "Validation Loss: 0.929834921692693\n",
      "Epoch 893, Loss: 0.1821525451878822\n",
      "Validation Loss: 1.0934988904831022\n",
      "Epoch 894, Loss: 0.18088701614287012\n",
      "Validation Loss: 1.4557549800983696\n",
      "Epoch 895, Loss: 0.18147239769094212\n",
      "Validation Loss: 1.068701917348906\n",
      "Epoch 896, Loss: 0.18135652626149876\n",
      "Validation Loss: 1.235416203737259\n",
      "Epoch 897, Loss: 0.1808092728322155\n",
      "Validation Loss: 0.7557933614697567\n",
      "Epoch 898, Loss: 0.18217821008807353\n",
      "Validation Loss: 4.652275573375613\n",
      "Epoch 899, Loss: 0.18118726842235341\n",
      "Validation Loss: 0.9076016053210857\n",
      "Epoch 900, Loss: 0.18175965177198483\n",
      "Validation Loss: 1.2974747280741847\n",
      "Epoch 901, Loss: 0.18194261782391127\n",
      "Validation Loss: 0.7550863024107245\n",
      "Epoch 902, Loss: 0.1809309860908015\n",
      "Validation Loss: 1.2746767353179842\n",
      "Epoch 903, Loss: 0.18038917181276998\n",
      "Validation Loss: 1.3068698679291926\n",
      "Epoch 904, Loss: 0.1809677972003471\n",
      "Validation Loss: 1.0777093470096588\n",
      "Epoch 905, Loss: 0.18039521468846603\n",
      "Validation Loss: 0.7092439975849417\n",
      "Epoch 906, Loss: 0.18026848239174417\n",
      "Validation Loss: 0.8262458041656849\n",
      "Epoch 907, Loss: 0.1811153753552326\n",
      "Validation Loss: 0.7716972391272701\n",
      "Epoch 908, Loss: 0.18026287398885849\n",
      "Validation Loss: 1.1095865521320076\n",
      "Epoch 909, Loss: 0.18061078246682882\n",
      "Validation Loss: 0.602403828571009\n",
      "Epoch 910, Loss: 0.18201675941777784\n",
      "Validation Loss: 0.8231186308833056\n",
      "Epoch 911, Loss: 0.18066517394685813\n",
      "Validation Loss: 0.5998256975828216\n",
      "Epoch 912, Loss: 0.18057071844245806\n",
      "Validation Loss: 1.271832782168721\n",
      "Epoch 913, Loss: 0.17939112177335245\n",
      "Validation Loss: 1.0307706927144251\n",
      "Epoch 914, Loss: 0.18071151643904837\n",
      "Validation Loss: 0.8991873416789743\n",
      "Epoch 915, Loss: 0.17955313173534218\n",
      "Validation Loss: 1.592694459959518\n",
      "Epoch 916, Loss: 0.18021835200488567\n",
      "Validation Loss: 0.9937571647555329\n",
      "Epoch 917, Loss: 0.18106433031214184\n",
      "Validation Loss: 0.6423749140528745\n",
      "Epoch 918, Loss: 0.1799972440917478\n",
      "Validation Loss: 0.7407331036966901\n",
      "Epoch 919, Loss: 0.18135411662677692\n",
      "Validation Loss: 4.0208727093630054\n",
      "Epoch 920, Loss: 0.18188166607517836\n",
      "Validation Loss: 0.5947660644387089\n",
      "Epoch 921, Loss: 0.18028374936778185\n",
      "Validation Loss: 1.3109844548757685\n",
      "Epoch 922, Loss: 0.18015055786168507\n",
      "Validation Loss: 1.1635172540365264\n",
      "Epoch 923, Loss: 0.18099298902115848\n",
      "Validation Loss: 1.093078971602196\n",
      "Epoch 924, Loss: 0.17986356417193664\n",
      "Validation Loss: 1.1456696800021238\n",
      "Epoch 925, Loss: 0.1795404249325741\n",
      "Validation Loss: 1.5150617568991904\n",
      "Epoch 926, Loss: 0.1804282704021695\n",
      "Validation Loss: 1.2329204928043276\n",
      "Epoch 927, Loss: 0.1798501605557841\n",
      "Validation Loss: 2.7561455438303395\n",
      "Epoch 928, Loss: 0.18090728467850145\n",
      "Validation Loss: 1.0701408268407333\n",
      "Epoch 929, Loss: 0.1824527739677145\n",
      "Validation Loss: 3.726108811622442\n",
      "Epoch 930, Loss: 0.17991655543013368\n",
      "Validation Loss: 1.2361201124135839\n",
      "Epoch 931, Loss: 0.18036000708897792\n",
      "Validation Loss: 1.282062635172245\n",
      "Epoch 932, Loss: 0.18097900685875914\n",
      "Validation Loss: 1.7275946680889573\n",
      "Epoch 933, Loss: 0.18031150789195022\n",
      "Validation Loss: 1.276187643755314\n",
      "Epoch 934, Loss: 0.18012103788164813\n",
      "Validation Loss: 1.1673910506936007\n",
      "Epoch 935, Loss: 0.18063532822050674\n",
      "Validation Loss: 0.7881105313467425\n",
      "Epoch 936, Loss: 0.18060968927822488\n",
      "Validation Loss: 1.2829916005910829\n",
      "Epoch 937, Loss: 0.1794333848127619\n",
      "Validation Loss: 1.3775494057078694\n",
      "Epoch 938, Loss: 0.18044774002547181\n",
      "Validation Loss: 0.884265580149584\n",
      "Epoch 939, Loss: 0.17968121602005044\n",
      "Validation Loss: 1.0819591512513715\n",
      "Epoch 940, Loss: 0.1796427829364358\n",
      "Validation Loss: 1.5723029818645744\n",
      "Epoch 941, Loss: 0.17872736925735724\n",
      "Validation Loss: 0.6101401396269022\n",
      "Epoch 942, Loss: 0.18941287857718592\n",
      "Validation Loss: 1.999344215836636\n",
      "Epoch 943, Loss: 0.1825204751952443\n",
      "Validation Loss: 1.0966545939445496\n",
      "Epoch 944, Loss: 0.18163302428154057\n",
      "Validation Loss: 1.4178990139517673\n",
      "Epoch 945, Loss: 0.18144398667784625\n",
      "Validation Loss: 1.8463211752647577\n",
      "Epoch 946, Loss: 0.1801057223577139\n",
      "Validation Loss: 0.8724519180697065\n",
      "Epoch 947, Loss: 0.18019985333951408\n",
      "Validation Loss: 0.875650979058687\n",
      "Epoch 948, Loss: 0.17914064633551724\n",
      "Validation Loss: 1.5003980744716734\n",
      "Epoch 949, Loss: 0.1802167218871588\n",
      "Validation Loss: 0.8873007547023685\n",
      "Epoch 950, Loss: 0.1786617710536649\n",
      "Validation Loss: 0.5288510773071023\n",
      "Epoch 951, Loss: 0.18000504487129146\n",
      "Validation Loss: 0.4764268374027208\n",
      "Epoch 952, Loss: 0.17933651745362683\n",
      "Validation Loss: 0.8394462556339973\n",
      "Epoch 953, Loss: 0.17891789128094218\n",
      "Validation Loss: 0.8278737165207086\n",
      "Epoch 954, Loss: 0.1807373240048629\n",
      "Validation Loss: 1.8876766992169758\n",
      "Epoch 955, Loss: 0.1798968956004395\n",
      "Validation Loss: 1.1964221416517746\n",
      "Epoch 956, Loss: 0.1791548240864866\n",
      "Validation Loss: 1.6022977648779404\n",
      "Epoch 957, Loss: 0.17936272887772944\n",
      "Validation Loss: 0.883000849984413\n",
      "Epoch 958, Loss: 0.17924778661582358\n",
      "Validation Loss: 1.1381878381551698\n",
      "Epoch 959, Loss: 0.17961414465897305\n",
      "Validation Loss: 1.0177622520646383\n",
      "Epoch 960, Loss: 0.1785474324620567\n",
      "Validation Loss: 1.3914002193961033\n",
      "Epoch 961, Loss: 0.178300982135413\n",
      "Validation Loss: 0.9092318949311279\n",
      "Epoch 962, Loss: 0.17871245387725013\n",
      "Validation Loss: 0.9181015131085418\n",
      "Epoch 963, Loss: 0.1778561010263687\n",
      "Validation Loss: 0.7762213820634887\n",
      "Epoch 964, Loss: 0.17972966782792016\n",
      "Validation Loss: 1.0732858264169027\n",
      "Epoch 965, Loss: 0.17973027774674255\n",
      "Validation Loss: 0.7183503612529399\n",
      "Epoch 966, Loss: 0.1778589137396667\n",
      "Validation Loss: 0.7438647656939751\n",
      "Epoch 967, Loss: 0.17902332833469953\n",
      "Validation Loss: 0.971811632084292\n",
      "Epoch 968, Loss: 0.18099464454449887\n",
      "Validation Loss: 2.8951309254003124\n",
      "Epoch 969, Loss: 0.18656071262477442\n",
      "Validation Loss: 0.5859524635381477\n",
      "Epoch 970, Loss: 0.18270959402975995\n",
      "Validation Loss: 0.863683087188144\n",
      "Epoch 971, Loss: 0.18517996032917222\n",
      "Validation Loss: 5.418454453002575\n",
      "Epoch 972, Loss: 0.1839284623951413\n",
      "Validation Loss: 1.424771060777265\n",
      "Epoch 973, Loss: 0.18192336552364882\n",
      "Validation Loss: 1.3817568282748378\n",
      "Epoch 974, Loss: 0.1788661455042487\n",
      "Validation Loss: 1.1913025503934815\n",
      "Epoch 975, Loss: 0.17932571804280892\n",
      "Validation Loss: 0.7809881990732148\n",
      "Epoch 976, Loss: 0.17947817845053451\n",
      "Validation Loss: 0.69867996213048\n",
      "Epoch 977, Loss: 0.17965928451049812\n",
      "Validation Loss: 0.7744920884453973\n",
      "Epoch 978, Loss: 0.1787119605903362\n",
      "Validation Loss: 0.6836689322493797\n",
      "Epoch 979, Loss: 0.17847291707212842\n",
      "Validation Loss: 0.9416563247525415\n",
      "Epoch 980, Loss: 0.1779847200959921\n",
      "Validation Loss: 0.8866584238617919\n",
      "Epoch 981, Loss: 0.17812356413450353\n",
      "Validation Loss: 0.8329012393951416\n",
      "Epoch 982, Loss: 0.17821942744126848\n",
      "Validation Loss: 0.9593570925468622\n",
      "Epoch 983, Loss: 0.17759060228330104\n",
      "Validation Loss: 1.0211770201838293\n",
      "Epoch 984, Loss: 0.1792605325666278\n",
      "Validation Loss: 1.1987451065418333\n",
      "Epoch 985, Loss: 0.17790249397241792\n",
      "Validation Loss: 1.0270882595417112\n",
      "Epoch 986, Loss: 0.179825987526064\n",
      "Validation Loss: 2.717005585515222\n",
      "Epoch 987, Loss: 0.17925916481113366\n",
      "Validation Loss: 0.9498638018619182\n",
      "Epoch 988, Loss: 0.17765636606748367\n",
      "Validation Loss: 0.5848601810460867\n",
      "Epoch 989, Loss: 0.1773444709887858\n",
      "Validation Loss: 0.8057744918867599\n",
      "Epoch 990, Loss: 0.17760425554805023\n",
      "Validation Loss: 1.1289114383764045\n",
      "Epoch 991, Loss: 0.17793478743108207\n",
      "Validation Loss: 1.3064761369727378\n",
      "Epoch 992, Loss: 0.1780644507840449\n",
      "Validation Loss: 0.879645366308301\n",
      "Epoch 993, Loss: 0.17691232229388038\n",
      "Validation Loss: 1.0491098165512085\n",
      "Epoch 994, Loss: 0.17960576708753442\n",
      "Validation Loss: 0.678733813208203\n",
      "Epoch 995, Loss: 0.1814614851027727\n",
      "Validation Loss: 0.4022027423908544\n",
      "Epoch 996, Loss: 0.17867893035805157\n",
      "Validation Loss: 1.0609481279240098\n",
      "Epoch 997, Loss: 0.177179426680384\n",
      "Validation Loss: 0.8264797426933466\n",
      "Epoch 998, Loss: 0.1772612014915361\n",
      "Validation Loss: 1.274090674727462\n",
      "Epoch 999, Loss: 0.17749791064971063\n",
      "Validation Loss: 1.5147313087485557\n",
      "Epoch 1000, Loss: 0.17792942817864377\n",
      "Validation Loss: 0.7163667626852213\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class MultiClassNNWithBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiClassNNWithBatchNorm, self).__init__()\n",
    "        self.fc1 = nn.Linear(16, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)  # BatchNorm for the first hidden layer\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)   # BatchNorm for the second hidden layer\n",
    "        self.fc3 = nn.Linear(64, 7)     # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model with batch normalization\n",
    "model = MultiClassNNWithBatchNorm()\n",
    "\n",
    "# Assuming you have train_dl_DB and test_dl_DB\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dl_DB:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_dl_DB)}\")\n",
    "\n",
    "    validation_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dl_DB:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            validation_loss += loss.item()\n",
    "    print(f\"Validation Loss: {validation_loss / len(test_dl_DB)}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7bf63065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.82\n",
      "Accuracy: 0.82\n",
      "AUC: 0.98\n"
     ]
    }
   ],
   "source": [
    "test_loss, f1, accuracy, auc = eval_db(model, test_dl_DB)\n",
    "print_metric(\"F1 score\", f1)\n",
    "print_metric(\"Accuracy\", accuracy)\n",
    "print_metric(\"AUC\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ec783c-e350-473e-b3e1-140095ebed53",
   "metadata": {},
   "source": [
    "## Counterfactual Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb92884-22cd-46e4-824d-8354e35ce856",
   "metadata": {},
   "source": [
    "### Designing a Distance Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f2aac-ad49-4a47-a74a-21158879f06d",
   "metadata": {},
   "source": [
    "**Task 3(a)**: First, we need to specify a suitable distance metric for measuring the closeness between the points. Depending on the dataset, one could choose the standard distance functions like the Manhattan (L1) distance, Euclidean (L2) distance, and more specialised ones like Gower distance for better handling datasets with both categorical and continuous dataset. However, the design of distance metric can be very flexible. For example, the standard L1 distance is (k is the number of features) $$d_{L1}(x, x') = \\sum_{i}^{k} |x_i-x'_i|$$\n",
    "\n",
    "If the features have different value ranges, we could normalise the L1 distance with the maximum and minimum values of each feature (indexed $i$) in the training dataset, $max_i$, $min_i$ : $$d_{L1, normalised}(x, x') = \\sum_{i}^{k} |(x_i-x'_i)/(max_i-min_i)|$$\n",
    "\n",
    "On top of this, we could also add customised weighting factors $\\mathbf{w}=w_1, ..., w_k$ to capture the importance of each feature, and the weighted L1 distance is: $$d_{L1, normalised, weighted}(x, x') = \\sum_{i}^{k} w_i|(x_i-x'_i)/(max_i-min_i)|$$\n",
    "\n",
    "Given the background above, we want to design a distance function for the preprocessed version of our Titanic dataset. Explore the dataset characteristics and answer the following questions:\n",
    "**a)** Briefly discuss the weighting of each input variable in the preprocessed dataset, if we use standard L1 and normalised L1?\n",
    "**b)** If we want to treat each feature equally in the original unprocessed dataset, how would you design the distance metric for the preprocessed dataset using L1-based distance? Write down the detail of your distance function for the preprocessed dataset and justify why each original feature is treated equally.\n",
    "**c)** Implement your distance function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b077fa1-b082-438a-806b-119714ca33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_function(x1, x2):\n",
    "    \"\"\"\n",
    "    Your distance function.\n",
    "\n",
    "    Parameters:\n",
    "        x1 (Tensor): A 1-d array of shape (k,)\n",
    "        x2 (Tensor): A 1-d array of shape (k,)\n",
    "\n",
    "    Returns:\n",
    "        distance (float): A real number >= 0\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df345ad0-6bdb-47b9-abf9-598c23a963c4",
   "metadata": {},
   "source": [
    "### Nearest-Neighbour Counterfactual Explanations (NNCE)\n",
    "\n",
    "**Task 3(b)**: As introduced in the tutorial, NNCEs are a simple yet effective method for finding counterfactuals. Implement the NNCE functions.\n",
    "\n",
    "Instructions:\n",
    "1. determine desired label for the counterfactual\n",
    "2. find the dataset points with desired label as predicted by the model\n",
    "3. find the point with the minimum distance and return it as NNCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47409a30-fbad-4371-99d6-b87d30cd6590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nnce(x, m, train_set, dist):\n",
    "    \"\"\"\n",
    "    Function to compute NNCE.\n",
    "\n",
    "    Parameters:\n",
    "        x (Tensor): Input, a 1-d array of shape (k,)\n",
    "        m (Sequential): Our neural network\n",
    "        train_set (TitanicDataset): Our Titanic dataset\n",
    "        dist (function): Your previously implemented distance function\n",
    "\n",
    "    Returns:\n",
    "        nnce (Tensor): Nearest neighbour counterfactual explanation, an 1-d array of shape (k,)\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07e25d4e-eb20-4a62-a468-64ce6834d6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0050], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# now compute NNCE and print out the NNCE's prediction result. Ideally this is different from the result for the original input.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m nnce \u001b[38;5;241m=\u001b[39m compute_nnce(test_input, model, train_set\u001b[38;5;241m=\u001b[39mtrain_dataset, dist\u001b[38;5;241m=\u001b[39mdistance_function)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnnce\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/AIETHic/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/AIETHic/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/AIETHic/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/AIETHic/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/AIETHic/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/AIETHic/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "# use this code block to test if your function is working\n",
    "# first print out the original input's prediction result\n",
    "test_input = test_dataset.samples.to(DEVICE)[0]\n",
    "print(model(test_input))\n",
    "\n",
    "# now compute NNCE and print out the NNCE's prediction result. Ideally this is different from the result for the original input.\n",
    "nnce = compute_nnce(test_input, model, train_set=train_dataset, dist=distance_function)\n",
    "print(model(nnce))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6290a-7c2b-4064-8ed0-3a4056bac9dc",
   "metadata": {},
   "source": [
    "### Gradient-Based Counterfactual Explanations\n",
    "\n",
    "**Task 3(c)**: Complete the PyTorch implementation for the gradient-based method in [Wachter et al. 2017]: WAC.\n",
    "\n",
    "Instructions:\n",
    "1. We are going to optimise the following loss function to find a counterfactual x': $  argmin_{x'} \\text{ } BCE(y', (1-y)) + \\lambda cost(x, x')$, where $BCE$ is binary cross entropy loss, $y'$ is the predicted label of $x'$, $y$ is the predicted label of $x$, $cost(,)$ is your chosen distance function in Task 1, and $\\lambda$ is the trade-off parameter between validity and proximity. First, implement your chosen distance metric in ```CostLoss.forward()```\n",
    "2. Follow the code structure in the ```compute_wac()``` function, complete the implementation.\n",
    "    2.1. specify the target label for the counterfactual\n",
    "    2.2. implement gradient descent procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6aae81-d6bb-4037-810d-198c00143334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CostLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(CostLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        The PyTorch version of your distance function\n",
    "\n",
    "        Parameters:\n",
    "            x1 (Tensor): A 1-d array of shape (k,)\n",
    "            x2 (Tensor): A 1-d array of shape (k,)\n",
    "\n",
    "        Returns:\n",
    "            distance (Tensor): a real number\n",
    "        \"\"\"\n",
    "        # TODO: Here is an example of standard L1 loss, replace it with your designed distance function in Task 1\n",
    "        dist = torch.abs(x1 - x2)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ae1f7-8c47-4a71-a817-2f22d48a64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "import datetime\n",
    "\n",
    "def compute_wac(x, m, lamb=0.1, lr=0.01, max_iter=1000, max_allowed_minutes=0.5):\n",
    "    \"\"\"\n",
    "    Function to find WAC using gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "        x (Tensor): Input x, an 1-d array of shape (k,)\n",
    "        m (Sequential): PyTorch model\n",
    "        lamb (float): Lambda, the tradeoff term in the loss function\n",
    "        lr (float): Learning rate for gradient descent\n",
    "        max_iter (int): maximum allowed iteration\n",
    "        max_allowed_minutes (float): maximum allowed minutes\n",
    "\n",
    "    Returns:\n",
    "        counterfactual (Tensor): Counterfactual point, an 1-d array of shape (k,)\n",
    "    \"\"\"\n",
    "    # initialise the counterfactual search at the input point\n",
    "    x = x.to(DEVICE)\n",
    "    wac = Variable(x.clone(), requires_grad=True).to(DEVICE)\n",
    "\n",
    "    # initialise an optimiser for gradient descent over the wac counterfactual point\n",
    "    optimiser = Adam([wac], lr, amsgrad=True)\n",
    "\n",
    "    # instantiate the two components of the loss function\n",
    "    validity_loss = torch.nn.BCELoss()\n",
    "    cost_loss = CostLoss()\n",
    "\n",
    "    # TASK: specify target label y: either 0 or 1, depending on the original prediction\n",
    "    # TODO: Start your code here\n",
    "\n",
    "    # this line below is a placeholder, change this\n",
    "    y_target = torch.Tensor([1]).to(DEVICE)\n",
    "\n",
    "    # the total loss in the instructions: loss = validity_loss + lamb * cost_loss\n",
    "    # TODO: End your code here\n",
    "\n",
    "    # compute class probability\n",
    "    class_prob = m(wac)\n",
    "    wac_valid = False\n",
    "    iterations = 0\n",
    "    if y_target == 0 and class_prob < 0.5 or y_target == 1 and class_prob >= 0.5:\n",
    "        wac_valid = True\n",
    "\n",
    "    # set maximum allowed time for computing 1 counterfactual\n",
    "    t0 = datetime.datetime.now()\n",
    "    t_max = datetime.timedelta(minutes=max_allowed_minutes)\n",
    "    # start gradient descent\n",
    "    while not wac_valid or iterations <= max_iter:\n",
    "\n",
    "        # TASK: gradient descent to find wac\n",
    "        # TODO: Your code here\n",
    "\n",
    "        # break conditions: valid counterfactual found, or iterations exceeded, or reached allowed max time\n",
    "        if y_target == 0 and class_prob < 0.5 or y_target == 1 and class_prob >= 0.5:\n",
    "            wac_valid = True\n",
    "        if datetime.datetime.now() - t0 > t_max:\n",
    "            break\n",
    "        iterations += 1\n",
    "\n",
    "    return wac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3730ddc2-6dbe-46a2-8bf3-e833800c52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this code block to test if your function is working\n",
    "# first print out the original input's prediction result\n",
    "test_input = test_dataset.samples.to(DEVICE)[1]\n",
    "print(model(test_input))\n",
    "\n",
    "# now compute WAC and print out the WAC's prediction result. Ideally this is different from the result for the original input.\n",
    "nnce = compute_wac(test_input, model)\n",
    "print(model(nnce))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb7d395-b999-4100-85dd-7b5eed5c57d7",
   "metadata": {},
   "source": [
    "### Performance of the Two Methods\n",
    "\n",
    "**Task 3(d)**: In order to understand better how these two methods compare, we use the following metrics to quantitatively evaluate each of the methods:\n",
    "- Validity: percentage of the counterfactuals that are valid.\n",
    "- Proximity: average distance between the counterfactuals and the inputs. Smaller distance (lower cost) indicates better proximity.\n",
    "- Plausibility: average distance of a counterfactual to its 5 nearest neighbours in the training dataset, further averaged over all counterfactuals. The closer it is to the nearest neighbours, the more plausible. Consider this metric as a simplified version of Local Outlier Factor.\n",
    "\n",
    "For each counterfactual method, we randomly select 20 test inputs, generate counterfactuals for them, and compare the average performances for each of the metrics.\n",
    "We repeat this process for 5 times and calculate the mean and standard deviation of each metrics. We have provided code for these experiments. Complete the following codes for calculating the evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4847e8d0-c76c-462d-a986-ccd2dc300e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_three_metrics_for_group_of_inputs(inputs, m, counterfactuals, train_set, dist):\n",
    "    validity, proximity, plausibility = 0, 0, 0\n",
    "    # examine validity, proximity, plausibility for each input-counterfactual pair\n",
    "    for i, x in enumerate(inputs):\n",
    "        ce = counterfactuals[i]\n",
    "\n",
    "        this_val = calculate_validity(x, ce, m)\n",
    "        this_prox = calculate_proximity(x, ce, dist)\n",
    "        this_plaus = calculate_plausibility(ce, train_set, dist)\n",
    "\n",
    "        validity += this_val\n",
    "        proximity += this_prox\n",
    "        plausibility += this_plaus\n",
    "\n",
    "    # average evaluation metrics over all the test inputs\n",
    "    validity = validity / len(inputs)\n",
    "    proximity = proximity / len(inputs)\n",
    "    plausibility = plausibility / len(inputs)\n",
    "    return validity, proximity, plausibility\n",
    "\n",
    "\n",
    "# TASK: for each input-counterfactual pair, calculate validity, proximity, and plausibility\n",
    "\n",
    "# check whether a counterfactual ce is valid or not\n",
    "def calculate_validity(x, ce, m):\n",
    "    # TODO: Your code here\n",
    "    pass\n",
    "\n",
    "# check the distance between a counterfactual and the input\n",
    "def calculate_proximity(x, ce, dist):\n",
    "    # TODO: Your code here\n",
    "    pass\n",
    "\n",
    "# calculate plausibility of a counterfactual\n",
    "def calculate_plausibility(ce, train_set, dist):\n",
    "    # here, calculate the average distance between the counterfactual and its 5 nearest neighbours in the training dataset\n",
    "    \n",
    "    # TODO: Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1cb9d3-cb0b-4473-b7d2-166e76032c61",
   "metadata": {},
   "source": [
    "Now we set up the experiments, compute counterfactuals using NNCE and WAC, then evaluate and compare their performances. Note that depending on your machine, the computation for ```compute_wac()``` could potentially be slow. You can also manually change the function's hyperparameters ```lamb=0.1, lr=0.01, max_iter=1000``` to try and see if WAC could give better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10f578-a107-4fc5-8a33-246018d9b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we store the evaluation results of the five runs in lists\n",
    "nnce_validity, nnce_proximity, nnce_plausibility = [], [], []\n",
    "wac_validity, wac_proximity, wac_plausibility = [], [], []\n",
    "seed_num = 1000\n",
    "\n",
    "# repeat over 5 runs to obtain more robust evaluations\n",
    "for one_run in tqdm(range(5)):\n",
    "    # randomly select 20 test inputs\n",
    "    np.random.seed(seed_num)\n",
    "    test_inputs = test_dataset.samples[np.random.choice(range(len(test_dataset.samples)), 20)]\n",
    "    nnce_counterfactuals, wac_counterfactuals = [], []\n",
    "    # generate counterfactuals\n",
    "    with tqdm(total=1, position=0, leave=True) as pbar:\n",
    "        for x in tqdm(test_inputs, position=0, leave=True):\n",
    "            nnce_counterfactuals.append(compute_nnce(x, model, train_dataset, distance_function))\n",
    "            wac_counterfactuals.append(compute_wac(x, model, lamb=0.1))\n",
    "            pbar.update()\n",
    "\n",
    "    # evaluate counterfactuals\n",
    "    nnvalidity, nnproximity, nnplausibility = calculate_three_metrics_for_group_of_inputs(test_inputs, model,\n",
    "                                                                                          nnce_counterfactuals,\n",
    "                                                                                          train_dataset,\n",
    "                                                                                          distance_function)\n",
    "    nnce_validity.append(nnvalidity)\n",
    "    nnce_proximity.append(nnproximity)\n",
    "    nnce_plausibility.append(nnplausibility)\n",
    "\n",
    "    wvalidity, wproximity, wplausibility = calculate_three_metrics_for_group_of_inputs(test_inputs, model,\n",
    "                                                                                       wac_counterfactuals,\n",
    "                                                                                       train_dataset, distance_function)\n",
    "    wac_validity.append(wvalidity)\n",
    "    wac_proximity.append(wproximity)\n",
    "    wac_plausibility.append(wplausibility)\n",
    "\n",
    "    seed_num += 10\n",
    "\n",
    "# now print out the results\n",
    "from tabulate import tabulate\n",
    "\n",
    "score_names = [\"method\", \"validity\", \"cost\", \"plausibility\"]\n",
    "score_table = [score_names,\n",
    "               [\"NNCE\", f\"{np.mean(nnce_validity).round(3)} +- {np.std(nnce_validity).round(3)}\",\n",
    "                f\"{np.mean(nnce_proximity).round(3)} +- {np.std(nnce_proximity).round(3)}\",\n",
    "                f\"{np.mean(nnce_plausibility).round(3)} +- {np.std(nnce_plausibility).round(3)}\"],\n",
    "               [\"WAC\", f\"{np.mean(wac_validity).round(3)} +- {np.std(wac_validity).round(3)}\",\n",
    "                f\"{np.mean(wac_proximity).round(3)} +- {np.std(wac_proximity).round(3)}\",\n",
    "                f\"{np.mean(wac_plausibility).round(3)} +- {np.std(wac_plausibility).round(3)}\"]]\n",
    "print(tabulate(score_table, headers='firstrow', tablefmt='outline'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2474358c-0c66-444a-b528-d4e99f1d4e78",
   "metadata": {},
   "source": [
    "### Performance Differences\n",
    "\n",
    "**Task 3(e)**: Discuss in your report commenting their performance based on the metrics. Link the findings to their theories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
